{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve information of the variants affecting the positions 23x49(BS), 12x48 (IBS, chain B interaction), 6x29, 4x39 (IBS), and 3x50 (IBS, CM Ionic Lock)\n",
    "We have previous analyzed the sensitive regions of GPCRs:\n",
    "1. Binding Site: we have defined the residues using a dataset of human aminergic GPCR crystallyzed with ligand. Then, analysing which of the 46 GPCRs (we had complete information of the variants affecting those GPCRs coming from the Hauser aper) presented variants in each of the positions that form the BS we selected position 23x49, as 14/46 GPCRs presented a non-homolous variant there.\n",
    "2. Conserved Motif: we have defined three CM using the positions of GPCRmd, NaBS, IL and RTS. Then again, we have analyzed which of the 46 GPCRs had non-homologous variants in those positions. The most interesting one was 3x50 (IL) with 17/46. It is also part of the IBS.\n",
    "3. Intracellular Binding Site: defined with the class A human GPCRs crystallized wth intracellular ligand. Then, after the non-homolous variant analysis of the 46 GPCRs (apart from position 3x50 that was already selected) we took 12x48, that was sometimes interacting with the secondary chain of the GPCRs and is present in 15/46 (though there were other two positions with 15/46). And also 6x29 and 4x39.\n",
    "\n",
    "Now, we need to search for those positions 12x48, 23x49, 3x50, 4x39, 6x29, the variants information as we will have to select the most interesting ones to simulate. We would base our selection in its frequency (related with conservation) in the population (or computing the impact scores of the Hauser paper Sift, Polyphen) and the possible relation with a disease. \n",
    "\n",
    "How to retrieve this information? First, as the GPCRdb (used by Houser) is not updated as we checked in retrieve_variants_info_GPCRdb.ipynb, we will gather the MV information of all the GPCRs from GnomAD (previously ExAC, used by Mariona, now is not available). We will use the variant information (allele frequency, count, number and position) and, on the other hand, we will also base the variant selection in the possible relation with a disease using DisGeNET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (0.25.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: scipy in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from scipy) (1.18.5)\n",
      "Requirement already satisfied: bioservices in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (1.7.6)\n",
      "Requirement already satisfied: requests-cache in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (0.5.2)\n",
      "Requirement already satisfied: suds-jurko in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (0.6)\n",
      "Requirement already satisfied: grequests in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (0.6.0)\n",
      "Requirement already satisfied: easydev>=0.9.36 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (0.9.38)\n",
      "Requirement already satisfied: appdirs in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (1.4.4)\n",
      "Requirement already satisfied: colorlog in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (4.1.0)\n",
      "Requirement already satisfied: requests in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (2.24.0)\n",
      "Requirement already satisfied: lxml in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (4.5.2)\n",
      "Requirement already satisfied: pandas in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (0.25.3)\n",
      "Requirement already satisfied: wrapt in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (1.12.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (4.9.1)\n",
      "Requirement already satisfied: xmltodict in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from bioservices) (0.12.0)\n",
      "Requirement already satisfied: gevent in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from grequests->bioservices) (20.6.2)\n",
      "Requirement already satisfied: colorama in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from easydev>=0.9.36->bioservices) (0.4.3)\n",
      "Requirement already satisfied: pexpect in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from easydev>=0.9.36->bioservices) (4.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from requests->bioservices) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from requests->bioservices) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from requests->bioservices) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from requests->bioservices) (1.25.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from pandas->bioservices) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from pandas->bioservices) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from pandas->bioservices) (2020.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from beautifulsoup4->bioservices) (2.0.1)\n",
      "Requirement already satisfied: setuptools in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from gevent->grequests->bioservices) (49.2.0.post20200712)\n",
      "Requirement already satisfied: zope.event in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from gevent->grequests->bioservices) (4.4)\n",
      "Requirement already satisfied: zope.interface in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from gevent->grequests->bioservices) (5.1.0)\n",
      "Requirement already satisfied: greenlet>=0.4.16; platform_python_implementation == \"CPython\" in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from gevent->grequests->bioservices) (0.4.16)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from pexpect->easydev>=0.9.36->bioservices) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas->bioservices) (1.15.0)\n",
      "Requirement already satisfied: biopython in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (1.77)\n",
      "Requirement already satisfied: numpy in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (from biopython) (1.18.5)\n",
      "Requirement already satisfied: xlrd in /home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages/htmd/versionwarnings.py:29: UserWarning: As of HTMD 1.16 the default ACEMD version for all protocols has changed to version 3. If you want to use version 2 protocols change the _version argument in the protocols or add `config(acemdversion=2)` to the beginning of your scripts. To disable this warning run once: `from htmd import _disableWarnings; _disableWarnings('1.16');`\n",
      "  , UserWarning)\n",
      "/home/martalo/miniconda3/envs/my_new_env_name/lib/python3.6/site-packages/htmd/versionwarnings.py:33: UserWarning: As of HTMD 1.16 the default number of threads HTMD spawns for calculations is set to 1. You can enable parallelism at your own risk using `config(njobs=-2)` in the beginning of your scripts. To disable this warning run once: `from htmd import _disableWarnings; _disableWarnings('1.16');`\n",
      "  , UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffevaluate module is in beta version\n",
      "\n",
      "Please cite HTMD: Doerr et al.(2016)JCTC,12,1845. https://dx.doi.org/10.1021/acs.jctc.6b00049\n",
      "\n",
      "HTMD Documentation at: https://www.htmd.org/docs/latest/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-01 10:26:33,049 - binstar - INFO - Using Anaconda API: https://api.anaconda.org\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New devel HTMD version (1.23.5 python[3.6,<3.7.0a0,3.7,<3.8.0a0]) is available. You are currently on (1.19).There are several methods to update:    - Create a new conda env. using `conda create -n htmd1.23.5 htmd=1.23.5 -c acellera -c psi4 -c conda-forge`    - Create a brand new conda installation and run `conda install htmd -c acellera -c psi4 -c conda-forge`    - Run: `conda update htmd -c acellera -c psi4 -c conda-forge` (NOT RECOMMENDED)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_project=\"/home/martalo/Documentos/TFM/GPCR_variants\"\n",
    "\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install scipy\n",
    "!{sys.executable} -m pip install bioservices\n",
    "!{sys.executable} -m pip install biopython\n",
    "!{sys.executable} -m pip install xlrd\n",
    "\n",
    "from common_functions import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import urllib\n",
    "\n",
    "import mdtraj as md\n",
    "import itertools\n",
    "config(viewer='webgl')\n",
    "from create_csv import * # this file is from Code/create_csv.py\n",
    "import requests\n",
    "import os\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdir =\"/home/martalo/Documentos/TFM/GPCR_variants/Code/potential_high_impact_variants/\"\n",
    "parentdir = \"/home/martalo/Documentos/TFM/GPCR_variants/\"\n",
    "datapath=\"/home/martalo/Documentos/TFM/GPCR_variants/Data/studied_GPCR_vars\"\n",
    "resultspath=\"/home/martalo/Documentos/TFM/GPCR_variants/Results/studied_GPCR_vars\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_excel_from_raw(file_family_raw, file_family_csv):\n",
    "    '''Read the txt file of the raw family information and build a ordered csv in the result \n",
    "    path'''\n",
    "    # open the raw file \n",
    "    first_line=True\n",
    "    with open(os.path.join(datapath,file_family_raw),\"r\") as infile:\n",
    "        # build a csv file with the ordered info: family, name, short name, UniPort Id of each of the structures\n",
    "        with open(os.path.join(resultspath,file_family_csv), 'w') as outfile:    \n",
    "            mywriter = csv.writer(outfile,delimiter=';')\n",
    "            for line in infile:\n",
    "                if first_line:\n",
    "                    mywriter.writerow(['Family','Name','Short','Uniprot ID'])\n",
    "                    first_line=False\n",
    "                if len(line) - len(line.lstrip())> 8 and not first_line:\n",
    "                    el_li=line.strip().split('\",\"')\n",
    "                    fam=el_li[2]\n",
    "                    short_nm=el_li[10]\n",
    "                    long_nm=el_li[11]\n",
    "                    uniprot=el_li[15]\n",
    "                    mywriter.writerow([fam,long_nm,short_nm,uniprot])\n",
    "    infile.close()\n",
    "    outfile.close()\n",
    "    return print('The file', file_family_csv,'has been succesfully created')\n",
    "\n",
    "def search_struc_pdbs(original_csv, extended_csv): \n",
    "    '''With the information of the family, name, short name and Uniport ID, create\n",
    "    another file adding the Uniprot entry name and the information of the structure\n",
    "    from pdb (three colums, if there is active, intermediate and active structure)'''\n",
    "    u = UniProt()# 'connection' with Uniprot services             \n",
    "    with open(os.path.join(resultspath,original_csv),\"r\") as infile:\n",
    "        with open(os.path.join(resultspath,extended_csv), 'w') as outfile:   \n",
    "            mywriter = csv.writer(outfile,delimiter=';') \n",
    "            myreader = csv.reader(infile, delimiter=';')\n",
    "            for row in myreader:\n",
    "                if row[0]==\"Family\":\n",
    "                    mywriter.writerow(row+[\"Uniprot entry\",\"Struc\"])\n",
    "                    continue\n",
    "                struccell=\"\"\n",
    "                entry=\"\"\n",
    "                uprot=row[3]\n",
    "                print(\"\\n\\n\",row[2])\n",
    "                if uprot:\n",
    "                    data=u.quick_search(\"id:%s\" % uprot)\n",
    "                    if data:\n",
    "                        entry=data[uprot]['Entry name'].lower()\n",
    "                        struc_res=obtain_struc_pdb(entry)\n",
    "                        if struc_res:\n",
    "                            struccell=struc_res\n",
    "                            print(struccell)\n",
    "                        else:\n",
    "                            temp=requests.get('http://gpcrdb.org/services/structure/template/'+entry).json()\n",
    "                            if temp:\n",
    "                                temp_res=obtain_struc_pdb(temp)\n",
    "                                if temp_res:\n",
    "                                    struccell=\"[Model]: \"+temp_res\n",
    "                                    print(struccell)\n",
    "                                else:\n",
    "                                    print(\"-----No struc for template\")\n",
    "                            else:\n",
    "                                print(\"-----No template\")\n",
    "                    else:\n",
    "                        print(\"-----Uprot ID not found\")\n",
    "                else:\n",
    "                    print(\"-----No uprot ID\")\n",
    "            \n",
    "                mywriter.writerow(row+[entry,struccell])\n",
    "    infile.close()\n",
    "    outfile.close()\n",
    "    return print('The file', extended_csv,'has been succesfully created')\n",
    "\n",
    "def add_colum_names(csv_file_new, csv_file_old):\n",
    "    '''Add colum names to the struc file and store it into new file'''\n",
    "    with open(os.path.join(resultspath,csv_file_old),\"r\") as strucfile_old:\n",
    "        with open(os.path.join(resultspath,csv_file_new),\"w\") as strucfile_new:\n",
    "            myreader= csv.reader(strucfile_old, delimiter=';')\n",
    "            mywriter= csv.writer(strucfile_new, delimiter=';') \n",
    "            for myrow in myreader:#iterate over the struc file rows\n",
    "                if myrow[0]==\"Familiy\":\n",
    "                    newrow=['Family', 'Name', 'Short', 'Uniprot ID','Uniprot entry','Struc','Struc1','Struc2']\n",
    "                    print(newrow)\n",
    "                    mywriter.writerow(newrow)\n",
    "                else:\n",
    "                    mywriter.writerow(myrow)\n",
    "    strucfile_new.close()\n",
    "    strucfile_old.close()\n",
    "    return None\n",
    "\n",
    "def create_csv_file(variants_excel, variants_csv):\n",
    "    '''Tranform an Excel xlsx file into a csv file'''\n",
    "    varFileAux=pd.read_excel(os.path.join(datapath,variants_excel))\n",
    "    varFileAux.to_csv(os.path.join(datapath,variants_csv), sep=\";\", index=False)\n",
    "    return None\n",
    "\n",
    "def merge_struc_cols(csv_new, csv_old):\n",
    "    '''Merged the possible three 'struc' pdb values, linked by '&' '''\n",
    "    mynewrow=[]\n",
    "    with open(os.path.join(resultspath,csv_old),\"r\") as file_old:\n",
    "        with open(os.path.join(resultspath,csv_new),\"w\") as file_new:\n",
    "            myreader= csv.reader(file_old, delimiter=';')\n",
    "            mywriter= csv.writer(file_new, delimiter=';') \n",
    "            for myrow in myreader:#iterate over the file rows\n",
    "                count=0\n",
    "                if myrow[0]==\"Family\":\n",
    "                    mywriter.writerow(myrow[:-2])\n",
    "                    continue\n",
    "                else: \n",
    "                    mynewrow=myrow\n",
    "                    mynewrow[5]=myrow[5].replace(',', ' &')\n",
    "                    mywriter.writerow(mynewrow)\n",
    "                    mynewrow=[]\n",
    "    file_new.close()\n",
    "    file_old.close()\n",
    "    return None\n",
    "\n",
    "def get_gene(entry_name):\n",
    "    '''Obtain the ENSEMBL id of the gene of the entry name'''\n",
    "    uprot_map=uniprot_mapping('ACC+ID', 'ENSEMBL_ID', entry_name)#tab-delimited output of the mapping\n",
    "    ens_id= uprot_map.decode().strip(\"\\n\").split(\"\\t\")[-1]\n",
    "    return ens_id\n",
    "\n",
    "def short_AA(AA_three_letters):\n",
    "    '''Tranform 3 letter AA to one letter AA'''\n",
    "    amin=AA_three_letters.upper()\n",
    "    d = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    "     'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N', \n",
    "     'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W', \n",
    "     'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
    "    return d[amin]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GPCR 'variants' file: GPCRdb index, GnomAD variants info, DisGeNET.\n",
    "It we be obtained with the index file from GPCRdb and the variant data from GnomAD plus DisgeNET.\n",
    "\n",
    "#### 1. Download  the txt file with the family information\n",
    "First, we need the file with the names of all the GPCRs (obtained from https://gpcrdb.org/mutational_landscape/statistics, 'proteins_and_families_rawALL.txt') to be an 'index'.\n",
    "\n",
    "Build the first csv (/Results) that will contain the family, name, short name and Uniprot ID of all the GPCRs that are in GPCRdb (https://gpcrdb.org/mutational_landscape/statistics) from the raw file/seed file 'proteins_and_families_rawALL.txt':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to csv the raw txt with the family/name/short name/Uniprot ID info \n",
    "#of all the GPCRs from GPCRdb\n",
    "\n",
    "# Only call if the previous csv have changed\n",
    "#create_excel_from_raw(\"proteins_and_families_rawALL.txt\", 'myprot_list_ALL.csv')\n",
    "\n",
    "###################NOTE##########################################################\n",
    "# Whenever one of the created csv files is manually edited with Libre Office it somehow \n",
    "#changes the format, so to edit it use the plain text editor!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Add the structure information in a new csv\n",
    "Build another csv file (/Results) that stores the same info as before plus the  Uniprot entry name and three possible extra columns with the pdb structures names (active, intermediate, inactive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only call if the previous csv have changed\n",
    "#search_struc_pdbs(\"myprot_list_ALL.csv\", 'myprot_list_struc_ALL_old.csv')\n",
    "\n",
    "# correct the column names\n",
    "#add_colum_names(\"myprot_list_struc_ALL.csv\", \"myprot_list_struc_ALL_old.csv\")\n",
    "\n",
    "# Put struc2 and struc3 in the same column linked by '&'\n",
    "#merge_struc_cols(\"myprot_list_struc_ALL_merged.csv\", \"myprot_list_struc_ALL.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Build the final 'variants_ALL_GNOMAD.csv' \n",
    "It will contain the GPCR pdb structure info plus the info of the variants that comes from GnomAD. We will use gnomAD v2 data set which contains data from 125,748 exomes and 15,708 whole genomes from unrelated individuals sequenced as part of various disease-specific and population genetic studies, totalling 141,456 individuals, and is aligned against the GRCh37 reference sequence (the latest version v3 has less exomes information, recommended if you are interesting in non-coding regions, not our case).\n",
    "https://gnomad.broadinstitute.org/\n",
    "\n",
    "First, we have an example on how to make queries in GnomAD, as they do not have an API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query GnomAD to get: hgvsp (to get segment position, original AA and variant AA), allele freq,\n",
    "#num, count and annotation from an Ensembl ID\n",
    "import pprint\n",
    "prettyprint = pprint.PrettyPrinter(indent=2).pprint\n",
    "\n",
    "def fetch(jsondata, url=\"https://gnomad.broadinstitute.org/api\"):\n",
    "    # The server gives a generic error message if the content type isn't\n",
    "    # explicitly set\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(url, json=jsondata, headers=headers)\n",
    "    json = response.json()\n",
    "    if \"errors\" in json:\n",
    "        raise Exception(str(json[\"errors\"]))\n",
    "    return json\n",
    "\n",
    "def get_variant_list(gene_id, dataset=\"gnomad_r2_1\"):\n",
    "    # Note that this is GraphQL, not JSON.\n",
    "    # Possible data to retrieve:\n",
    "    # pos\trsid\tref\talt\tconsequence\texome\tflags\tlof\tconsequence_in_canonical_transcript\n",
    "    #gene_symbol\thgvsc\tlof_filter\tlof_flags\thgvsp\treference_genome\tvariant_id\t\n",
    "    #genome_af\tgenome_ac\tgenome_an\tgenome_ac_hemi\tgenome_ac_hom\tgenome\texome_af\t\n",
    "    #exome_ac\texome_an\texome_ac_hemi\texome_ac_hom\n",
    "    \n",
    "    # We need the hgvsp (segNum, originalAA and variantAA), allele freq, allele num, allele \n",
    "    # count, consequence (get only MVs). We lack number of homozygotes genome_ac_hom\n",
    "    fmt_graphql = \"\"\"\n",
    "    {\n",
    "        gene(gene_id: \"%s\") {\n",
    "          variants(dataset: %s) {\n",
    "            hgvsp            \n",
    "            genome {\n",
    "                af\n",
    "                ac\n",
    "                an\n",
    "            }\n",
    "            consequence\n",
    "            rsid\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    \"\"\"# rsid, variant_id: variantId -> maybe useful\n",
    "    \n",
    "    # This part will be JSON encoded, but with the GraphQL part left as a\n",
    "    # glob of text.\n",
    "    req_variantlist = {\n",
    "        \"query\": fmt_graphql % (gene_id, dataset),\n",
    "        \"variables\": {}\n",
    "        }\n",
    "    response = fetch(req_variantlist)#json format\n",
    "    return response[\"data\"][\"gene\"][\"variants\"]\n",
    "    #return response\n",
    "\n",
    "# Examples of how the info is obtained, the results can be compared with the ones from\n",
    "# gnomad_python_api.git in the output folder, file: variants.tsv\n",
    "#prettyprint(get_variant_list(\"ENSG00000169174\"))\n",
    "# print(get_variant_list(\"ENSG00000135312\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve MV data of the GPCR of the index file\n",
    "\n",
    "def find_variants_gnomad(index_csv, final_csv):\n",
    "    '''Use the info of the index file to look for the variants stored in GnomAD and store\n",
    "    them in the final_csv'''\n",
    "    \n",
    "    non_identified=0 # keep track of the non-identified GPCRs\n",
    "    not_entry=0\n",
    "    with open(os.path.join(resultspath,index_csv),\"r\") as myprotfile:\n",
    "    # Build new file merging structure info (index file) + variants info (GnomAD), containing:\n",
    "    # Family\tName\tShort\tUniprot ID\tUniprot entry\tStruc\tLigandtype\tgene\n",
    "    #GPCRdb\tSequenceNumber\tSegment\tGPCRdbWT\tNMaa\tMutationType\tAllele Count\n",
    "    #Allele Frequency\tAllele Number\tNumber of Homozygotes\tsift_word\tsift_score\n",
    "    #polyphen_word\tpolyphen_score\tGProteinInteraction\tArrestinInteraction\t\n",
    "    #ActivationPathway\tMicroSwitch\tSodiumPocket\tfoldchangeMaxAbs\tdiseaseId\t\n",
    "    #score\tdiseaseName\tType\tPTMsite\tLB_structure\tLB_fam\n",
    "\n",
    "        with open(os.path.join(resultspath,final_csv), 'w') as outfile:   \n",
    "            myprotread = csv.reader(myprotfile, delimiter=';') # csv with struc info (index)\n",
    "            mywriter = csv.writer(outfile,delimiter=';') # final csv\n",
    "            for myrow in myprotread:#iterate over the struc file rows\n",
    "                if myrow[0]==\"Family\": # write first colum of the new csv\n",
    "                    newrow=myrow+['Ligandtype','gene','varID', 'GPCRdb', 'SequenceNumber', 'Segment', 'GPCRdbWT', 'NMaa', 'MutationType', 'Allele Count', 'Allele Frequency', 'Allele Number', 'Number of Homozygotes', 'sift_word', 'sift_score', 'polyphen_word', 'polyphen_score', 'GProteinInteraction', 'ArrestinInteraction', 'ActivationPathway', 'MicroSwitch', 'SodiumPocket', 'foldchangeMaxAbs', 'diseaseId', 'score', 'diseaseName', 'Type', 'PTMsite', 'LB_structure', 'LB_fam']\n",
    "                    mywriter.writerow(newrow)\n",
    "                    # Containing the elements of struc file plus the interesting ones of the var \n",
    "                    continue\n",
    "                entry=myrow[4] # colum 4 from csv with struc info: Uniprot entry\n",
    "                print('Entry',entry)\n",
    "                if not entry: # if there is not Uniprot entry\n",
    "                    mywriter.writerow(myrow)\n",
    "                    not_entry+=1\n",
    "                    continue\n",
    "                \n",
    "                ens_id= get_gene(entry) # get Ensembl ID\n",
    "                no_id=False\n",
    "                try: # now check this Ensembl ID in gnomAD\n",
    "                    gnom=get_variant_list(ens_id)  # list of dict of the variants found in\n",
    "                #gnomAD of the entry\n",
    "                except:\n",
    "                    print('No Ensemble ID for ', entry)\n",
    "                    no_id=True\n",
    "                    non_identified+=1\n",
    "                \n",
    "                # if the search in GnomAD has been succesfull\n",
    "                if not no_id:\n",
    "#                     print('Esemble ID correct')\n",
    "                    # get the structure data of the protein from GPCRdb\n",
    "                    gpcr_pos_info=get_gpcr_pos_info(entry)\n",
    "                    # keys as the 'sequence_number' (position) and values -> protein_segment, gnum\n",
    "\n",
    "                    found=False                    \n",
    "                    for gnom_var in gnom:#iterate over the found variants for the entry\n",
    "                        if gnom_var[\"consequence\"] ==\"missense_variant\":\n",
    "#                             print('MV in GnomAD')\n",
    "                            found=True\n",
    "                            \n",
    "                            var_id=gnom_var['rsid']\n",
    "                            \n",
    "                            # get the seqNum, originalAA and variantAA from 'hgvsp' \n",
    "                            AAdata=gnom_var['hgvsp']\n",
    "                            fromAA_long=AAdata[2:5] # Ex: Ala\n",
    "                            toAA_long=AAdata[-3:] # Ex: Gly\n",
    "                            fromAA=short_AA(fromAA_long) # Ex: A\n",
    "                            toAA=short_AA(toAA_long) # Ex: G\n",
    "                            seqNum=int(re.findall('\\d+', gnom_var['hgvsp'] )[0])# get only the numbers\n",
    "#                             print('seqNum:', seqNum)\n",
    "                            \n",
    "                            # get allele data\n",
    "                            if gnom_var[\"genome\"]:\n",
    "                                gen_dict=gnom_var[\"genome\"]\n",
    "                                allele_count=gen_dict['ac']\n",
    "                                allele_freq=gen_dict['af']\n",
    "                                allele_num=gen_dict['an']\n",
    "                            else:\n",
    "                                allele_count='?'\n",
    "                                allele_freq='?'\n",
    "                                allele_num='?'\n",
    "                            \n",
    "                            #look for the gnum of the variant position\n",
    "                            gpcrNum=\"\"\n",
    "                            try: \n",
    "                                this_pos_info=gpcr_pos_info[seqNum]\n",
    "                                if this_pos_info[\"display_generic_number\"]:\n",
    "                                    gpcrNum=this_pos_info[\"display_generic_number\"]\n",
    "                                Segment=this_pos_info[\"protein_segment\"]\n",
    "                            except:\n",
    "                                print('Position/SeqNum not found')                                \n",
    "                                gpcrNum='?'\n",
    "                                Segment='?'\n",
    "                                \n",
    "                            hom_count='?'###### Lack from GnomAD\n",
    "                            Ligandtype=\"?\"\n",
    "                            MutationType=check_aa_type_changed(fromAA,toAA)\n",
    "                            sift_word=\"?\"\n",
    "                            sift_score=\"?\"\n",
    "                            polyphen_word=\"?\"\n",
    "                            polyphen_score=\"?\"\n",
    "                            GProteinInteraction=\"?\"\n",
    "                            ArrestinInteraction=\"?\"\n",
    "                            ActivationPathway=\"?\"\n",
    "                            MicroSwitch=\"?\"\n",
    "                            SodiumPocket=\"?\"\n",
    "                            foldchangeMaxAbs=\"?\"\n",
    "                            diseaseId=\"?\"\n",
    "                            score=\"?\"\n",
    "                            diseaseName=\"?\"\n",
    "                            Type=\"?\"\n",
    "                            PTMsite=\"?\"\n",
    "                            LB_structure=\"?\"\n",
    "                            LB_fam=\"?\"\n",
    "                            \n",
    "                            mywriter.writerow(myrow + [Ligandtype, ens_id, var_id, gpcrNum, seqNum, Segment, fromAA, toAA, MutationType, allele_count, allele_freq, allele_num, hom_count, sift_word, sift_score, polyphen_word, polyphen_score, GProteinInteraction, ArrestinInteraction, ActivationPathway, MicroSwitch, SodiumPocket, foldchangeMaxAbs, diseaseId, score, diseaseName, Type, PTMsite, LB_structure, LB_fam])\n",
    "                    if not found:\n",
    "                        print(\"No variants found for %s\" % entry)\n",
    "                        mywriter.writerow(myrow)\n",
    "    print('Without ensembl ID:', non_identified)\n",
    "    print('Without entry:', not_entry)\n",
    "    return None\n",
    "\n",
    "# find_variants_gnomad(\"myprot_list_struc_ALL_merged.csv\",\"variants_ALL_GNOMAD.csv\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the variants\n",
    "#### 1. Filter the data with allele information and affecting the positions of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_var_file(var_all, var_filtered, pos_to_filter):\n",
    "    '''Filter the file with all the variants downloaded from GnomAD to only\n",
    "    get the ones with allele fre, num, count and affecting the interesting positions'''\n",
    "    with open(os.path.join(resultspath,var_all),\"r\") as myprotfile:\n",
    "        with open(os.path.join(resultspath,var_filtered), 'w') as outfile:   \n",
    "            myprotread = csv.reader(myprotfile, delimiter=';') # csv with struc info (index)\n",
    "            mywriter = csv.writer(outfile,delimiter=';') # final csv\n",
    "            for myrow in myprotread:#iterate over the struc file rows\n",
    "                if myrow[0]==\"Family\": # write first colum of the new csv\n",
    "                    mywriter.writerow(myrow)                \n",
    "                    continue\n",
    "                else:\n",
    "                    try: # if the gnum is different than '?'\n",
    "                        gnum=gnum_all_to_one(\"gpcrdb\",myrow[9])\n",
    "                    except:\n",
    "                        gnum=False\n",
    "                        continue\n",
    "\n",
    "                    allele=[myrow[15], myrow[16], myrow[17]] # allele freq, num, count            \n",
    "                    info_ok= True\n",
    "                    for element in allele:\n",
    "                        if element == '?':\n",
    "                            info_ok=False\n",
    "                    if info_ok and gnum in pos_to_filter:\n",
    "                        mynewrow=myrow\n",
    "                        mynewrow[9]=gnum\n",
    "                        \n",
    "                        #DisGeNET info\n",
    "                        disCols=[myrow[29],myrow[30],myrow[31]]\n",
    "                        varIDrs=myrow[8]\n",
    "                        \n",
    "                        \n",
    "                        mywriter.writerow(mynewrow)\n",
    "\n",
    "    return (print(\"The file\",var_filtered, \"has been succesfully created.\"))\n",
    "\n",
    "interest_pos=['23x49','12x48','4x39','6x29', '3x50'] # BS, IBS (3), CM-IL (also of IBS)\n",
    "\n",
    "# filter_var_file(\"variants_ALL_GNOMAD.csv\", \"variants_ALL_GNOMAD_filtered.csv\", interest_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve information from DisGeNET\n",
    "Now, we want to get the information of DisGeNEt to fill in the information of columns: 'diseaseId', 'score', 'diseaseName' of the previous file. To do so, we will use the rest API disgenet.org/api/, specially, the Variant-Disease Associations (VDAs). The VDAs service provides an interface for accessing variant-disease associations from DisGeNET. The VDAs can be retrieved for a specific variant (ex: rs121909213), or list of variants. It is recommended that the list of query variants is not longer than 100 entities. The service returns the results formated in TSV, JSON, or XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_disgenet(varID):\n",
    "    '''Get variant relation with a disease from DisgeNet using the variant id (Ex:rs121909211)'''\n",
    "    # https://www.disgenet.org/api/#/VDA/vdaByVariant\n",
    "    base = 'https://www.disgenet.org/api'\n",
    "    tool = 'vda/variant'\n",
    "    url = base+'/'+tool+'/'+varID\n",
    "    response =  urllib.request.urlopen(url)\n",
    "    return (response.read())\n",
    "\n",
    "def get_disease_info(req_response):\n",
    "    '''Select only the three important values of the DisgeNET response for each variant,\n",
    "    Id, disease name and score'''\n",
    "    # Available information:\n",
    "# {\"variantid\":\"rs121909211\",\"gene_symbol\":\"TGFBI\",\"variant_dsi\":\"0.724\",\"variant_dpi\":\"0.2\",\n",
    "#  \"variant_consequence_type\":\"missense variant\",\"diseaseid\":\"C1275685\",\n",
    "#  \"disease_name\":\"Avellino corneal dystrophy\",\"disease_class\":\"C16;C11\",\n",
    "#  \"disease_class_name\":\"   Congenital, Hereditary, and Neonatal Diseases and Abnormalities;    \n",
    "#  Eye Diseases\",\"disease_type\":\"disease\",\"disease_semantic_type\":\"Disease or Syndrome\",\n",
    "#  \"score\":0.9,\"ei\":1.0,\"year_initial\":1998,\"year_final\":2019,\"source\":\"ALL\"}\n",
    "\n",
    "    req_list = json.loads(req_response)\n",
    "    list_diseases=[]\n",
    "    dis_dic={}\n",
    "    print('Elements of the returned list of related diseases:')\n",
    "    for element in req_list:\n",
    "        dis_dic['disease_id']=element['diseaseid']\n",
    "        dis_dic['score']=element['score']\n",
    "        dis_dic['diseaseName']=element['disease_name']\n",
    "        list_diseases.append(dis_dic)\n",
    "        print(dis_dic)\n",
    "        dis_dic={}\n",
    "    return list_diseases\n",
    "\n",
    "\n",
    "def major_disease(list_of_diseases):\n",
    "    '''Returns a tupple with the dictionary of the disease with the higest score and \n",
    "    the total number of related diseases'''\n",
    "    mvp=list_of_diseases[0] # most important disease\n",
    "    number_dis=len(list_of_diseases) # number of related diseases\n",
    "    for element in list_of_diseases:\n",
    "        if element['score'] > mvp['score'] :\n",
    "            mvp=element\n",
    "    print('The disease with higher score is:',mvp)\n",
    "    print('Number of related diseases:',number_dis)\n",
    "    return(mvp, number_dis)\n",
    "\n",
    "# Examples:\n",
    "# related_diseases= get_disease_info(request_disgenet('rs121909211')) #list of diseases\n",
    "# most_imp_dis=major_disease(related_diseases)\n",
    "# print(most_imp_dis[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each varID may be related with more than one disease, we will store the complete information of the related disease with highest score but also the total number of related diseases for each variant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disCols=[29,30,31]  'diseaseId'-> 'diseaseid', 'score' -> 'score' (Score VDA: Variant-Disease \n",
    "# Asociation Score),'diseaseName' -> 'disease_name'\n",
    "\n",
    "def add_dis_info(var_filtered, var_disgenet):\n",
    "    '''Add the information of DisGeNET, id, socre, name, and the number of related\n",
    "    diseases for each varID'''\n",
    "    with open(os.path.join(resultspath,var_filtered),\"r\") as myprotfile:\n",
    "        with open(os.path.join(resultspath,var_disgenet), 'w') as outfile:   \n",
    "            myprotread = csv.reader(myprotfile, delimiter=';') # csv with struc info (index)\n",
    "            mywriter = csv.writer(outfile,delimiter=';') # final csv\n",
    "            for myrow in myprotread:#iterate over the struc file rows\n",
    "                if myrow[0]==\"Family\": # write first colum of the new csv\n",
    "                    mynewrow=myrow\n",
    "                    mynewrow.append(\"related_diseases\")\n",
    "                    mywriter.writerow(mynewrow)                \n",
    "                    continue\n",
    "                else:\n",
    "                    varID=myrow[8]\n",
    "                    try:\n",
    "                        related_dis= get_disease_info(request_disgenet(varID)) #list of diseases\n",
    "                    except:\n",
    "                        print('No matches in DisGeNET for:', varID)\n",
    "                        mywriter.writerow(myrow)\n",
    "                        continue\n",
    "                    imp_info=major_disease(related_dis)\n",
    "                    most_imp_dis=imp_info[0]# disease info with the highest score\n",
    "                    num_dis=imp_info[1] # number of related diseases\n",
    "                    \n",
    "                    mynewrow=myrow\n",
    "                    mynewrow[29]=most_imp_dis['disease_id']\n",
    "                    mynewrow[30]=most_imp_dis['score']\n",
    "                    mynewrow[31]=most_imp_dis['diseaseName']\n",
    "                    mynewrow.append(num_dis)\n",
    "                mywriter.writerow(mynewrow) \n",
    "    myprotfile.close()\n",
    "    outfile.close()\n",
    "    return (print('The file', var_disgenet,'has been succesfully created'))\n",
    "                      \n",
    "# add_dis_info(\"variants_ALL_GNOMAD_filtered.csv\", \"variants_ALL_GNOMAD_DISGENET.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting file 'variants_ALL_GNOMAD_DISGENET.csv' in the 'Results' file contains all the info of the variants affenting all kind GPCRs that had information of the allele freq, number and count in GnomAd. In addition, the variants that have information on their relation with diseases stored in DisGeNET is also added. \n",
    "\n",
    "## Compute the Impact Scores: SIFT, PolyPhen\n",
    "#### 1. SIFT: Sorting Intolerant From Tolerant\n",
    "It predicts whether an amino acid substitution is likely to affect protein function based on sequence homology and the physico-chemical similarity between the alternate amino acids. The data that Ensembl provides for each amino acid substitution is a score and a qualitative prediction (either 'tolerated' or 'deleterious'). The score is the normalized probability that the amino acid change is tolerated so scores nearer zero are more likely to be deleterious. The qualitative prediction is derived from this score such that substitutions with a score < 0.05 are called 'deleterious' and all others are called 'tolerated'. \n",
    "\n",
    "#### 2. PolyPhen: \n",
    "It predicts the effect of an amino acid substitution on the structure and function of a protein using sequence homology, Pfam annotations, 3D structures from PDB where available, and a number of other databases and tools (including DSSP, ncoils etc.). As with SIFT, for each amino acid substitution where Ensembl has been able to calculate a prediction, it provides both a qualitative prediction (one of 'probably damaging', 'possibly damaging', 'benign' or 'unknown') and a score. The PolyPhen score represents the probability that a substitution is damaging, so values nearer one are more confidently predicted to be deleterious (note that this the opposite to SIFT). The qualitative prediction is based on the False Positive Rate of the classifier model used to make the predictions. \n",
    "\n",
    "In the case of SIFT we would need to use SIFT dbSNP that provides SIFT predictions for a list nonsynonymous SNPs (rsIDs) from NCBI's dbSNP database. But in order to use an API we will use the API rest form Ensembl VEP (Variant Effect Predictor) that will directly give us the SIFT and PolyPhen information we need.\n",
    "\n",
    "https://rest.ensembl.org/documentation/info/vep_id_post\n",
    "https://sift.bii.a-star.edu.sg/www/SIFT_dbSNP.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def request_VEP(varID):\n",
    "    '''Get variant SIFT and PolyPhen2 from VEP (Ensembl) using the variant id (Ex:rs121909211)'''\n",
    "    server = \"https://rest.ensembl.org\"\n",
    "    ext = \"/vep/human/id/\"+varID+\"?\"\n",
    "\n",
    "    r = requests.get(server+ext, headers={ \"Content-Type\" : \"application/json\"})\n",
    "\n",
    "    if not r.ok:\n",
    "        r.raise_for_status()\n",
    "        sys.exit()\n",
    "\n",
    "    decoded = r.json()\n",
    "#     print(decoded)\n",
    "    return(decoded)\n",
    "\n",
    "# Examples:\n",
    "# response=request_VEP(\"rs56116432\")\n",
    "# response=request_VEP(\"rs199805893\")\n",
    "# response=request_VEP(\"rs759659059\")\n",
    "\n",
    "def filter_VEP_response(VEPresponse,ensID,varID):\n",
    "    '''Filer the respose, list type, from VEP, after quering a varID. Get the \n",
    "    'sift_prediction','sift_score','polyphen_prediction','polyphen_score' for \n",
    "    each transcriptID (after checking is the correct EnsemblID)'''\n",
    "#     pred_dic={}\n",
    "    keys=['sift_prediction','sift_score','polyphen_prediction','polyphen_score']\n",
    "    for element in VEPresponse:\n",
    "        prediction_list=element['transcript_consequences']\n",
    "        pred_dic={}\n",
    "        for dictionary in prediction_list:\n",
    "            predictions=True\n",
    "#             print(dictionary)\n",
    "#             print(dictionary['gene_id'], ensID)\n",
    "            if dictionary['gene_id']==ensID:# check ensemblID\n",
    "#                 print('ensID ok')\n",
    "                transID=dictionary['transcript_id'] # store each transcriptID\n",
    "#                 print('transID')\n",
    "                for value in keys:# check all the predictions exist\n",
    "                    if value not in dictionary.keys():\n",
    "                        predictions=False  \n",
    "#                         print('Not all the keys in the dictionary')\n",
    "                    if predictions:\n",
    "                        pred_dic[transID]={'Sift': [ dictionary['sift_score'], dictionary['sift_prediction'] ],\n",
    "                                            'Polyphen':[dictionary['polyphen_score'], dictionary['polyphen_prediction'] ]}\n",
    "            else:\n",
    "                print('ensid not ok')\n",
    "    # returned dictionary with keys as transcripID and values as the prediction values \n",
    "    return pred_dic\n",
    "\n",
    "# print(filter_VEP_response(response, \"ENSG00000258839\",\"rs34090186\"))\n",
    "# print(filter_VEP_response(response, \"ENSG00000185149\",\"rs199805893\"))\n",
    "\n",
    "\n",
    "def add_sift_polyphen(old,new):\n",
    "    '''Retrieve the information from VEP using the varID and EnsID to fill in the score \n",
    "    impact columns for SIFT and PolyPhen. As there could be more than one transcript\n",
    "    for the same geneID and varID store all the values separed by white spaces'''\n",
    "    with open(os.path.join(resultspath,old),\"r\") as myprotfile:\n",
    "        with open(os.path.join(resultspath,new), 'w') as outfile:   \n",
    "            myprotread = csv.reader(myprotfile, delimiter=';') \n",
    "            mywriter = csv.writer(outfile,delimiter=';') # final csv\n",
    "            for myrow in myprotread:\n",
    "                if myrow[0]==\"Family\": # write first colum of the new csv\n",
    "                    mynewrow=myrow\n",
    "                    mywriter.writerow(myrow)                \n",
    "                    continue\n",
    "                elif myrow[8]!='':\n",
    "                    mynewrow=myrow\n",
    "                    #check ensemblID and varID  to get the info\n",
    "                    # keys=['sift_prediction','sift_score','polyphen_prediction','polyphen_score']\n",
    "                    ensID=myrow[7]\n",
    "                    varID=myrow[8]\n",
    "                    print(varID)\n",
    "                    VEPresp=request_VEP(varID)#list (long info from VEP)\n",
    "                    filteredVEPresp=filter_VEP_response(VEPresp,ensID,varID)# dictionary \n",
    "                    #19,20,21,22\n",
    "#                     print(varID,filteredVEPresp)\n",
    "                    \n",
    "                    if len(filteredVEPresp.keys())==1:\n",
    "                        for transID in filteredVEPresp:#only one\n",
    "                            auxDic=filteredVEPresp[transID]\n",
    "                            siftData=auxDic['Sift']\n",
    "                            mynewrow[20]=(transID+':'+str(siftData[0]))\n",
    "                            mynewrow[19]=(transID+':'+siftData[1])\n",
    "                            ppData=auxDic['Polyphen']\n",
    "                            mynewrow[22]=(transID+':'+str(ppData[0]))\n",
    "                            mynewrow[21]=(transID+':'+ppData[1])\n",
    "                    elif len(filteredVEPresp.keys())>1:# more than one transcript\n",
    "                        for transID in filteredVEPresp:\n",
    "                            auxDic=filteredVEPresp[transID]\n",
    "                            siftData=auxDic['Sift']\n",
    "                            ppData=auxDic['Polyphen']\n",
    "                            if mynewrow[19]=='?' and mynewrow[20] == '?':\n",
    "                                mynewrow[20]=(transID+':'+str(siftData[0])+'   ')\n",
    "                                mynewrow[19]=(transID+':'+siftData[1]+'   ')\n",
    "                                mynewrow[22]=(transID+':'+str(ppData[0])+'   ')\n",
    "                                mynewrow[21]=(transID+':'+ppData[1]+'   ')\n",
    "                            else:\n",
    "                                mynewrow[20]+=(transID+':'+str(siftData[0])+'   ')\n",
    "                                mynewrow[19]+=(transID+':'+siftData[1]+'   ')\n",
    "                                mynewrow[22]+=(transID+':'+str(ppData[0])+'   ')\n",
    "                                mynewrow[21]+=(transID+':'+ppData[1]+'   ')\n",
    "                    mywriter.writerow(mynewrow)\n",
    "                    \n",
    "        return print('The file', new, 'has been succesfully created.')\n",
    "                    \n",
    "# add_sift_polyphen(\"variants_ALL_GNOMAD_DISGENET.csv\", \"variants_ALL_GNOMAD_DISGENET_PRED.csv\")                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # manage VEP responses\n",
    "# first=response[0]\n",
    "# for element in first:\n",
    "#     print(element,':')\n",
    "#     print(first[element],'\\n')\n",
    "# print('\\n')\n",
    "# sec=response[1]\n",
    "# for element in first:\n",
    "#     print(element,':')\n",
    "#     print(sec[element],'\\n')   \n",
    "    \n",
    "# count=0\n",
    "# keys=['sift_prediction','sift_score','polyphen_prediction','polyphen_score','transcript_id']\n",
    "# for element in response:\n",
    "    \n",
    "#     prediction_list=element['transcript_consequences']\n",
    "#     for dictionary in prediction_list:\n",
    "#         count+=1\n",
    "#         print('Reponse element number:', count)\n",
    "#         for key, value in dictionary.items():\n",
    "#             if key in keys:\n",
    "#                 print(key,'\\t',value)\n",
    "#     print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange variant information\n",
    "1. Affecting selected positions: 23x49, 12x48, 4x39, 6x29 or 3x50 -> DONE\n",
    "2. Probable functional impact: SIFT 'deleterious', PolyPhen 'Probably damaging'-> 0,1,2 DONE\n",
    "3. Related with at least one disease -> DONE\n",
    "4. With PDB structure information and preferably with the active and inactive states -> DONE\n",
    "5. Variant in segment with 3D information. -> as we have selected the positions we know they are part of helixes 3 (TM3 3x50), 4 (TM4 4x39) and 6 (TM& 6x29) and intracelullar loop 1 (ICL1, 12x48) and extracellular loop 1 (ECL1 23x49).\n",
    "6. PTM information, PTMdb***\n",
    "\n",
    "#### Probable functional impact: SIFT 'deleterious', PolyPhen 'Probably damaging' and relation with diseases\n",
    "Change the way in which we see the impacts. Set '0' if both predictors are below the scores that set the variants as 'deleterious' (SIFT) and 'probably_damaging' (PolyPhen2). Set '1' if just one predictor catalogs the variant as damaging and '2' if both predictors do.\n",
    "\n",
    "*Note: the PolyPhen2 tag, 'possibly damaging' is not considered high impact score (approx 0.7).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_add(listPredictions, dicPredictions, position):\n",
    "    '''split the strings that come from the csv, colums of the predictions, by ':' and\n",
    "    them to a dictionary. Used by the function 'fuse_preds' '''\n",
    "    for element in listPredictions:\n",
    "        if element != '':\n",
    "            items=element.split(':')#[transID, predictionValue]\n",
    "            if position==0:#void dictionary\n",
    "                dicPredictions[items[0]]=[items[1]]\n",
    "            else:    \n",
    "                dicPredictions[items[0]].append(items[1])\n",
    "    return None\n",
    "\n",
    "def fuse_preds(siftScore,siftPred,ppScore,ppPred):\n",
    "    '''Build a dictionary with keys as transID and values as siftScore, siftPred,\n",
    "    ppScore, ppPred'''\n",
    "    returnDic={}\n",
    "    #sift\n",
    "    predListSift=siftPred.split('   ')# transID1:'deleterious'1, transID2:'deleterious'2,...\n",
    "    scoreListSift=siftScore.split('   ')# transID1:score1,...\n",
    "    #poplyphen\n",
    "    predListPP=ppPred.split('   ')# transID1:'deleterious'1, transID2:'deleterious'2,...\n",
    "    scoreListPP=ppScore.split('   ')# transID1:score1,...\n",
    "    lists=[predListSift,scoreListSift,predListPP,scoreListPP]\n",
    "    index=-1\n",
    "    for element in lists:\n",
    "        index+=1\n",
    "        split_and_add(element,returnDic,index)#split by ':'\n",
    "    return returnDic\n",
    "\n",
    "\n",
    "# examples:\n",
    "# sp=\"ENST00000216629:tolerated   ENST00000553356:deleterious   ENST00000611804:tolerated   \" \n",
    "# ss=\"ENST00000216629:0.06   ENST00000553356:0.04   ENST00000611804:0.06 \"\n",
    "# ppp=\"ENST00000216629:probably_damaging   ENST00000553356:probably_damaging   ENST00000611804:probably_damaging\"\n",
    "# pps=\"ENST00000216629:0.983   ENST00000553356:0.972   ENST00000611804:0.983   \"\n",
    "\n",
    "# sp=\"ENST00000555147:deleterious   ENST00000555427:deleterious   ENST00000639847:deleterious   \"\n",
    "# ss=\"ENST00000555147:0.03   ENST00000555427:0.03   ENST00000639847:0.03   \"\n",
    "# ppp=\"ENST00000555147:benign   ENST00000555427:benign   ENST00000639847:benign   \"\n",
    "# pps=\"ENST00000555147:0.248   ENST00000555427:0.342   ENST00000639847:0.248   \"\n",
    "# dicProva=fuse_preds(sp,ss,ppp,pps)\n",
    "\n",
    "def get_trans(entry_name):\n",
    "    '''Obtain the ENSEMBL transIDs list of the gene with the protein entry name.\n",
    "    Used by the function 'clean_trans' '''\n",
    "    uprot_map=uniprot_mapping('ACC+ID', 'ENSEMBL_TRS_ID', entry_name)#tab-delimited output of the mapping\n",
    "    lines=uprot_map.decode().split(\"\\n\")\n",
    "    tIDs=[]\n",
    "    for elements in lines:\n",
    "        if elements!='' and elements !='From\\tTo':\n",
    "            tIDs.append(elements.split(\"\\t\")[-1])\n",
    "    return tIDs\n",
    "#example\n",
    "# print(get_trans('Q13639'))\n",
    "\n",
    "def clean_trans(transPredDic, entryName):\n",
    "    '''Only keep the transcript of the correct protein'''\n",
    "    correctTrans=get_trans(entryName)# list of the two correct transcripts of the protein\n",
    "#     print('Correnct transID:',correctTrans)\n",
    "    correctDic={}\n",
    "#     print('Intro dic:',transPredDic)\n",
    "    for key, value in transPredDic.items():\n",
    "        if key in correctTrans:\n",
    "            correctDic[key]=value\n",
    "    return correctDic\n",
    "# example\n",
    "# print(clean_trans(dicProva, 'Q01726'))\n",
    "\n",
    "def set_role(myrow):\n",
    "    '''Set which is the function of the AA: BS, IBS, CM depending on the variant position '''\n",
    "    position=myrow[9]\n",
    "    bs=['23x49'] # selected position from BS\n",
    "    ibs=['12x48','4x39','6x29'] # IBS (3)\n",
    "    cm_il=['3x50'] # CM-IL (also of IBS)\n",
    "    if position in bs:\n",
    "        function= 'LB_structure'\n",
    "    elif position in ibs: \n",
    "        if position=='12x48':\n",
    "            function=\"GProtein/ArrestinInteraction (chain B)\"\n",
    "        else:\n",
    "            function=\"GProtein/ArrestinInteraction\"\n",
    "    else:\n",
    "        function='IonicLock'\n",
    "    return function\n",
    "\n",
    "def write_newvar_line(orgRow,impactPred):\n",
    "    '''New structure of the rows '''\n",
    "    newrow=[]\n",
    "    for i in range(0,26):\n",
    "        newrow.append('')\n",
    "    for i in range(0,6):\n",
    "        newrow[i]=orgRow[i]\n",
    "    for i in range(6,17):\n",
    "        newrow[i]=orgRow[i+1]\n",
    "    newrow[17]=set_role(orgRow)#role\n",
    "    newrow[18]=impactPred#impact prediction:0, 1 or 2\n",
    "    for i in range (19,22):#diseaseID,score, name\n",
    "        newrow[i]=orgRow[i+10]\n",
    "    newrow[22]=orgRow[-1]#numer of diseases\n",
    "    for i in range(23,26):#foldchange, ptm, ptmtype\n",
    "        newrow[i]='?'\n",
    "    return newrow\n",
    "\n",
    "def compute_impact(dictionary):\n",
    "    '''Check if the variant is predicted to have impact by one or two prediction tools '''\n",
    "    impacto=0\n",
    "    for trasid in dictionary:\n",
    "        listPrediction=dictionary[trasid]\n",
    "        if listPrediction[0]=='deleterious' and listPrediction[2]=='probably_damaging':\n",
    "            impacto=2#both predict impact\n",
    "        elif listPrediction[0]=='deleterious' or listPrediction[2]=='probably_damaging':\n",
    "            impacto=1\n",
    "    return impacto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "columNamesORG=[\"Family0\",\"Name0\",\"Short1\",\"Uniprot ID2\",\"Uniprot entry3\",\"Struc4\",\"Ligandtype5\",\n",
    "               \"gene6\",\"varID7\",\"GPCRdb8\",\"SequenceNumber9\",\"Segment10\",\"GPCRdbWT11\",\"NMaa12\",\n",
    "               \"MutationType13\",\"Allele Count14\",\"Allele Frequency15\",\"Allele Number16\",\n",
    "               \"Number of Homozygotes17\",\"sift_word18\",\"sift_score19\",\"polyphen_word20\",\n",
    "               \"polyphen_score21\",\"GProteinInteraction22\",\"ArrestinInteraction23\",\n",
    "               \"ActivationPathway24\",\"MicroSwitch25\",\"SodiumPocket26\",\"foldchangeMaxAbs27\",\n",
    "               \"diseaseId28\",\"score29\",\"diseaseName30\",\"Type31\",\"PTMsite32\",\n",
    "               \"LB_structure33\",\"LB_fam34\",\"related_diseases35\"]\n",
    "\n",
    "# OUT:\n",
    "# \"ligandType\" = row 6\n",
    "# \"Number of Homozygotes\" = row 18\n",
    "\n",
    "# \"activationPathway\"=row 24\n",
    "# is out because it is descrived as 'Class A activation pathway positions: '3x46','6x37','7x53' \n",
    "# and none of those positions are considered'\n",
    "\n",
    "# \"LB_fam\" = row 35 (ligand binding interaction observed in a crystal structure within \n",
    "#            the same receptor family)\n",
    "\n",
    "# REPLACED:\n",
    "# \"sift_word\",\"sift_score\",\"polyphen_word\",\"polyphen_score\"=row 19,20,21,22\n",
    "# -> to 'Impact prediction', 0,1,2 (none, one, both predict impact)\n",
    "\n",
    "# \"GProteinInteraction\",\"ArrestinInteraction\",\"MicroSwitch\",\"SodiumPocket\",\"LB_structure\"\n",
    "# = row22,23,25,26,34\n",
    "# those will be changed by 'Role': BS, IBS/ionicLock (CM),IBS(arrestin/gprotint/chainA/B)\n",
    "\n",
    "# DOUBTS:\n",
    "# \"foldchangeMaxAbs\"= maximum (absolute) observed foldchange in in vitro pharmacological experiments\n",
    "# \"PTMsite\" = PTM site (yes/no) \n",
    "# \"Type\" = PTM type (replaced by PTMtype)\n",
    "\n",
    "\n",
    "columNamesNEW=[\"Family\",\"Name\",\"Short\",\"Uniprot ID\",\"Uniprot entry\",\"Struc\",\"gene\",\"varID\",\n",
    "               \"GPCRdb\",\"SequenceNumber\",\"Segment\",\"GPCRdbWT\",\"NMaa\",\"MutationType\",\n",
    "               \"Allele Count\", \"Allele Frequency\",\"Allele Number\",\"Role\",\"Impact Prediction\",\n",
    "               \"diseaseId\",\"score\",\"diseaseName\",\"related_diseases\",\"foldchangeMaxAbs\",\"PTMsite\", \"PTMtype\"]\n",
    "\n",
    "\n",
    "# build new csv with new column structure\n",
    "def new_col_struc(old,new):\n",
    "    with open(old,\"r\") as myvarfile:\n",
    "        myvarread = csv.reader(myvarfile, delimiter=';') \n",
    "        with open(new,\"w\") as myvarfile_pred: \n",
    "            mywriter = csv.writer(myvarfile_pred, delimiter=';') \n",
    "            for myrow in myvarread:\n",
    "                if myrow[0]==\"Family\": #first row, new names\n",
    "                    mywriter.writerow(columNamesNEW)\n",
    "                    continue            \n",
    "                else:\n",
    "                    if myrow[20]!='?' and  myrow[22]!='?':# there are impact predictors\n",
    "                        #compute impact value (0,1,2)\n",
    "                        print(myrow[3])\n",
    "                        impact=0#default not impact\n",
    "                        predDicAux=fuse_preds(myrow[20],myrow[19],myrow[22],myrow[21])#siftScore,siftPred,ppScore,ppPred\n",
    "                        #now filter the transID of the ensID of the entry name\n",
    "                        predDic=clean_trans(predDicAux, myrow[3])#prediction Dic and prot name\n",
    "                        print(predDic)\n",
    "                        impact=compute_impact(predDic)# change to 1,2\n",
    "                        print(impact)\n",
    "                    else:# no impact information\n",
    "                        impact=None\n",
    "\n",
    "                    mynewrow=write_newvar_line(myrow,impact)\n",
    "                    mywriter.writerow(mynewrow)\n",
    "    return None\n",
    "                    \n",
    "variants_aff_pos=os.path.join(resultspath,\"variants_ALL_GNOMAD_DISGENET_PRED.csv\")\n",
    "variants_aff_pos_pred=os.path.join(resultspath,\"filtered_var_list/vars_with_impact.csv\")\n",
    "\n",
    "# new_col_struc(variants_aff_pos,variants_aff_pos_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P08908', 'P28222', 'P28221', 'P28566', 'P30939', 'P28223', 'P41595', 'P28335', 'Q13639', 'P47898', 'P50406', 'P34969', 'P11229', 'P08172', 'P20309', 'P08173', 'P08912', 'P35348', 'P35368', 'P25100', 'P08913', 'P18089', 'P18825', 'P08588', 'P07550', 'P13945', 'P21728', 'P14416', 'P35462', 'P21917', 'P21918', 'P35367', 'P25021', 'Q9Y5N1', 'Q9H3N8', 'Q96RJ0', 'P30556', 'P50052', 'P35414', 'P28336', 'P30550', 'P32247', 'P46663', 'P30411', 'P32238', 'P32239', 'Q16581', 'P21730', 'Q9P296', 'P25101', 'P24530', 'P21462', 'P25090', 'P25089', 'P47211', 'O43603', 'O60755', 'Q92847', 'P30968', 'Q969F8', 'Q99705', 'Q969V1', 'Q01726', 'Q01718', 'P41968', 'P32245', 'P33032', 'O43193', 'Q9HB89', 'Q9GZQ4', 'Q9GZQ6', 'Q9Y5X5', 'Q6W5P4', 'P48145', 'P48146', 'P25929', 'P49146', 'P50391', 'Q15761', 'Q99463', 'P30989', 'O95665', 'P41143', 'P41145', 'P35372', 'P41146', 'O43613', 'O43614', 'Q96P65', 'P49683', 'P25116', 'P55085', 'O00254', 'Q96RI0', 'Q9HBX9', 'Q8WXD0', 'Q9NSD7', 'Q8TDU9', 'P30872', 'P30874', 'P32745', 'P31391', 'P35346', 'P25103', 'P21452', 'P29371', 'P34981', 'Q9UKP6', 'P37288', 'P47901', 'P30518', 'P30559', 'Q99788', 'P32246', 'P41597', 'P51677', 'P51679', 'P51681', 'P51684', 'P32248', 'P51685', 'P51686', 'P46092', 'P25024', 'P25025', 'P49682', 'P61073', 'P32302', 'O00574', 'P49238', 'P46094', 'Q16570', 'O00590', 'P25106', 'Q9NPB9', 'O00421', 'P23945', 'P22888', 'P16473', 'Q8TCW9', 'Q8NFJ6', 'O14842', 'O15552', 'O14843', 'Q5NUL3', 'O15529', 'Q15722', 'Q9NPC1', 'Q9Y271', 'Q9NS75', 'Q8TDS5', 'P25090', 'Q92633', 'Q9HBW0', 'Q9UBY5', 'Q99677', 'Q9H1C0', 'P43657', 'P21453', 'O95136', 'Q99500', 'O95977', 'Q9H228', 'P21554', 'P34972', 'Q14330', 'Q9Y2T6', 'Q8TDV5', 'P25105', 'Q13258', 'Q9Y5Y4', 'P34995', 'P43116', 'P43115', 'P35408', 'P43088', 'P43119', 'P21731', 'P48039', 'P49286', 'P30542', 'P29274', 'P29275', 'P0DMS8', 'P47900', 'P41231', 'P51582', 'Q15077', 'Q96G91', 'Q9H244', 'Q9BPV8', 'Q15391', 'Q8TDU6', 'Q99527', 'Q9BXC0', 'Q8TDS4', 'P49019', 'Q96P68', 'Q9BXA5', 'P08100', 'Q9UHM6', 'Q9H1Y3', 'Q6U736', 'P03999', 'P04001', 'P04000', 'P46091', 'P32247', 'P46089', 'P46093', 'O15529', 'P46095', 'P47775', 'P49685', 'Q13304', 'Q14330', 'Q15760', 'Q99678', 'Q99679', 'Q99680', 'O00155', 'Q8NDV2', 'Q9NS67', 'O00270', 'O75388', 'Q49SQ1', 'Q9UPC5', 'Q9HC97', 'O15354', 'O60883', 'O43194', 'Q9Y5Y3', 'Q13585', 'Q9Y2T5', 'Q9Y2T6', 'Q9BZJ8', 'Q9BZJ7', 'Q9BZJ6', 'Q8IYL9', 'Q15743', 'O95800', 'Q96P69', 'Q96P67', 'Q9NYM4', 'Q9NQS5', 'P60893', 'Q9BY21', 'Q9GZN0', 'Q96P66', 'Q8TDV5', 'Q9UNW8', 'Q8IZ08', 'Q6DWJ6', 'Q7Z602', 'Q7Z601', 'Q96CH1', 'Q8TDV2', 'Q86SP6', 'Q8NGU9', 'Q8TDV0', 'Q8TDT2', 'Q6NV75', 'Q9UJ42', 'Q8N6U8', 'Q16538', 'O14626', 'Q9NS66', 'Q9BXC1', 'Q14439', 'O15218', 'P32249', 'Q9BXB1', 'O75473', 'Q9HBX8', 'P04201', 'P35410', 'Q8TDS7', 'Q86SM8', 'Q96AM1', 'Q86SM5', 'Q96LB2', 'Q96LB1', 'Q96LB0', 'Q96LA9', 'Q9H1Y3', 'Q9UHM6', 'Q6U736', 'Q86VZ1', 'O00398', 'Q9P1P5', 'O14804', 'Q96RI8', 'Q969N4', 'Q96RI9']\n"
     ]
    }
   ],
   "source": [
    "def get_trans(entry_name):\n",
    "    '''Obtain the ENSEMBL transIDs list of the gene with the protein entry name.\n",
    "    Used by the function 'clean_trans' '''\n",
    "    uprot_map=uniprot_mapping('ACC+ID', 'ENSEMBL_TRS_ID', entry_name)#tab-delimited output of the mapping\n",
    "    lines=uprot_map.decode().split(\"\\n\")\n",
    "    tIDs=[]\n",
    "    for elements in lines:\n",
    "        if elements!='' and elements !='From\\tTo':\n",
    "            tIDs.append(elements.split(\"\\t\")[-1])\n",
    "    return tIDs\n",
    "#example\n",
    "# print(get_trans('Q13639'))\n",
    "\n",
    "def read_prots(fileProt):\n",
    "    listProts=[]\n",
    "    count=0\n",
    "    with open(os.path.join(resultspath,fileProt),\"r\") as myvarfile:\n",
    "        myvarread = csv.reader(myvarfile, delimiter=',') \n",
    "        for myrow in myvarread:\n",
    "            if myrow[0]==\"Family\": #first row, new names\n",
    "                continue            \n",
    "            else:\n",
    "                if myrow[3]=='P30988-2':\n",
    "                    break\n",
    "                else:\n",
    "                    listProts.append(myrow[3])\n",
    "    myvarfile.close()\n",
    "#     print(listProts)\n",
    "    return listProts\n",
    "\n",
    "protList=read_prots(os.path.join(resultspath,'myprot_list_struc_ALL.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transProtDic={}# key: UniprotName, value:[transIDs]\n",
    "for element in protList:\n",
    "    transProtDic[element]=get_trans(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P08908': ['ENST00000323865'], 'P28222': ['ENST00000369947'], 'P28221': ['ENST00000374619'], 'P28566': ['ENST00000305344'], 'P30939': ['ENST00000319595'], 'P28223': ['ENST00000378688', 'ENST00000542664', 'ENST00000543956'], 'P41595': ['ENST00000258400'], 'P28335': ['ENST00000276198', 'ENST00000371950', 'ENST00000371951'], 'Q13639': ['ENST00000360693', 'ENST00000362016', 'ENST00000377888', 'ENST00000517929', 'ENST00000520514', 'ENST00000521530', 'ENST00000521735', 'ENST00000522588', 'ENST00000631296'], 'P47898': ['ENST00000287907'], 'P50406': ['ENST00000289753'], 'P34969': ['ENST00000277874', 'ENST00000336152', 'ENST00000371719'], 'P11229': ['ENST00000306960', 'ENST00000543973'], 'P08172': ['ENST00000320658', 'ENST00000401861', 'ENST00000445907', 'ENST00000453373'], 'P20309': ['ENST00000255380', 'ENST00000615928'], 'P08173': ['ENST00000433765'], 'P08912': ['ENST00000383263', 'ENST00000557872'], 'P35348': ['ENST00000276393', 'ENST00000354550', 'ENST00000380572', 'ENST00000380573', 'ENST00000380582', 'ENST00000380586', 'ENST00000519096', 'ENST00000521711'], 'P35368': ['ENST00000306675'], 'P25100': ['ENST00000379453'], 'P08913': ['ENST00000280155'], 'P18089': ['ENST00000620793'], 'P18825': ['ENST00000330055'], 'P08588': ['ENST00000369295'], 'P07550': ['ENST00000305988'], 'P13945': ['ENST00000345060'], 'P21728': ['ENST00000393752'], 'P14416': ['ENST00000346454', 'ENST00000362072', 'ENST00000538967', 'ENST00000542968'], 'P35462': ['ENST00000383673', 'ENST00000460779', 'ENST00000467632'], 'P21917': ['ENST00000176183', 'ENST00000611962'], 'P21918': ['ENST00000304374'], 'P35367': ['ENST00000397056', 'ENST00000431010', 'ENST00000438284'], 'P25021': ['ENST00000231683', 'ENST00000377291'], 'Q9Y5N1': ['ENST00000340177'], 'Q9H3N8': ['ENST00000256906', 'ENST00000426880'], 'Q96RJ0': ['ENST00000275216'], 'P30556': ['ENST00000349243', 'ENST00000404754', 'ENST00000461609', 'ENST00000474935', 'ENST00000475347', 'ENST00000497524'], 'P50052': ['ENST00000371906'], 'P35414': ['ENST00000257254', 'ENST00000606794', 'ENST00000611099'], 'P28336': ['ENST00000258042'], 'P30550': ['ENST00000380289'], 'P32247': ['ENST00000370648'], 'P46663': ['ENST00000216629', 'ENST00000611804'], 'P30411': ['ENST00000539359', 'ENST00000542454', 'ENST00000554311'], 'P32238': ['ENST00000295589'], 'P32239': ['ENST00000334619', 'ENST00000525462'], 'Q16581': ['ENST00000307637'], 'P21730': ['ENST00000355085'], 'Q9P296': ['ENST00000595464', 'ENST00000600626'], 'P25101': ['ENST00000324300', 'ENST00000358556', 'ENST00000506066', 'ENST00000510697', 'ENST00000511804', 'ENST00000648866', 'ENST00000651419'], 'P24530': ['ENST00000377211', 'ENST00000475537', 'ENST00000626030', 'ENST00000646605', 'ENST00000646607', 'ENST00000646948'], 'P21462': ['ENST00000304748', 'ENST00000595042'], 'P25090': ['ENST00000340023', 'ENST00000598776', 'ENST00000598953'], 'P25089': ['ENST00000339223', 'ENST00000595991'], 'P47211': ['ENST00000299727'], 'O43603': ['ENST00000329003'], 'O60755': ['ENST00000249041'], 'Q92847': ['ENST00000241256', 'ENST00000427970'], 'P30968': ['ENST00000226413', 'ENST00000420975'], 'Q969F8': ['ENST00000234371'], 'Q99705': ['ENST00000249016'], 'Q969V1': ['ENST00000281806', 'ENST00000369212'], 'Q01726': ['ENST00000555147', 'ENST00000639847'], 'Q01718': ['ENST00000327606'], 'P41968': ['ENST00000243911'], 'P32245': ['ENST00000299766'], 'P33032': ['ENST00000324750', 'ENST00000589410'], 'O43193': ['ENST00000218721'], 'Q9HB89': ['ENST00000305141'], 'Q9GZQ4': ['ENST00000255262'], 'Q9GZQ6': ['ENST00000277942'], 'Q9Y5X5': ['ENST00000308744', 'ENST00000344413', 'ENST00000358749', 'ENST00000395999'], 'Q6W5P4': ['ENST00000359791', 'ENST00000360581', 'ENST00000381539', 'ENST00000381542', 'ENST00000381544', 'ENST00000381553', 'ENST00000396095', 'ENST00000465305', 'ENST00000531252'], 'P48145': ['ENST00000331251'], 'P48146': ['ENST00000369768', 'ENST00000612646'], 'P25929': ['ENST00000296533'], 'P49146': ['ENST00000329476', 'ENST00000506608'], 'P50391': ['ENST00000374312', 'ENST00000395716', 'ENST00000612632'], 'Q15761': ['ENST00000338566', 'ENST00000506953', 'ENST00000515560'], 'Q99463': [], 'P30989': ['ENST00000370501'], 'O95665': ['ENST00000306928'], 'P41143': ['ENST00000234961'], 'P41145': ['ENST00000265572', 'ENST00000520287', 'ENST00000524278', 'ENST00000612786', 'ENST00000613482'], 'P35372': ['ENST00000229768', 'ENST00000330432', 'ENST00000337049', 'ENST00000414028', 'ENST00000419506', 'ENST00000428397', 'ENST00000434900', 'ENST00000435918', 'ENST00000452687', 'ENST00000518759', 'ENST00000519083', 'ENST00000520708', 'ENST00000522236', 'ENST00000522555', 'ENST00000522739', 'ENST00000524150', 'ENST00000524163'], 'P41146': ['ENST00000336866', 'ENST00000349451', 'ENST00000355631'], 'O43613': ['ENST00000373706', 'ENST00000403528'], 'O43614': ['ENST00000370862', 'ENST00000615358'], 'Q96P65': ['ENST00000394427'], 'P49683': ['ENST00000239032', 'ENST00000636925'], 'P25116': ['ENST00000319211'], 'P55085': ['ENST00000296677'], 'O00254': ['ENST00000296641', 'ENST00000504899'], 'Q96RI0': ['ENST00000248076'], 'Q9HBX9': ['ENST00000307765', 'ENST00000343542', 'ENST00000470033', 'ENST00000471616'], 'Q8WXD0': ['ENST00000298386', 'ENST00000380314'], 'Q9NSD7': ['ENST00000330120', 'ENST00000616205'], 'Q8TDU9': ['ENST00000368318'], 'P30872': ['ENST00000267377'], 'P30874': ['ENST00000357585'], 'P32745': ['ENST00000610913', 'ENST00000617123'], 'P31391': ['ENST00000255008'], 'P35346': ['ENST00000293897'], 'P25103': ['ENST00000305249', 'ENST00000409848'], 'P21452': ['ENST00000373306'], 'P29371': ['ENST00000304883'], 'P34981': ['ENST00000311762', 'ENST00000518632'], 'Q9UKP6': ['ENST00000313135'], 'P37288': ['ENST00000299178'], 'P47901': ['ENST00000367126'], 'P30518': ['ENST00000337474', 'ENST00000370049', 'ENST00000646375'], 'P30559': ['ENST00000316793'], 'Q99788': ['ENST00000312143', 'ENST00000412676', 'ENST00000550402', 'ENST00000552995'], 'P32246': ['ENST00000296140'], 'P41597': ['ENST00000292301', 'ENST00000400888', 'ENST00000445132'], 'P51677': ['ENST00000357422', 'ENST00000395940', 'ENST00000395942', 'ENST00000545097'], 'P51679': ['ENST00000330953'], 'P51681': ['ENST00000292303', 'ENST00000445772'], 'P51684': ['ENST00000341935', 'ENST00000349984', 'ENST00000400926', 'ENST00000643861'], 'P32248': ['ENST00000246657'], 'P51685': ['ENST00000326306'], 'P51686': ['ENST00000355983', 'ENST00000357632', 'ENST00000395963'], 'P46092': ['ENST00000332438'], 'P25024': ['ENST00000295683'], 'P25025': ['ENST00000318507'], 'P49682': ['ENST00000373691', 'ENST00000373693'], 'P61073': ['ENST00000241393', 'ENST00000409817'], 'P32302': ['ENST00000292174'], 'O00574': ['ENST00000304552', 'ENST00000438735', 'ENST00000457814', 'ENST00000458629'], 'P49238': ['ENST00000358309', 'ENST00000399220', 'ENST00000541347', 'ENST00000542107'], 'P46094': ['ENST00000309285'], 'Q16570': ['ENST00000368121', 'ENST00000368122', 'ENST00000537147'], 'O00590': ['ENST00000422265', 'ENST00000442925'], 'P25106': ['ENST00000272928'], 'Q9NPB9': ['ENST00000249887'], 'O00421': ['ENST00000357392', 'ENST00000399036', 'ENST00000400880', 'ENST00000400882'], 'P23945': ['ENST00000304421'], 'P22888': ['ENST00000294954'], 'P16473': ['ENST00000342443', 'ENST00000554435'], 'Q8TCW9': ['ENST00000303786'], 'Q8NFJ6': ['ENST00000217270'], 'O14842': ['ENST00000246553'], 'O15552': ['ENST00000246549', 'ENST00000599180'], 'O14843': ['ENST00000327809', 'ENST00000594310'], 'Q5NUL3': ['ENST00000371481', 'ENST00000371483'], 'O15529': ['ENST00000454971', 'ENST00000597214'], 'Q15722': ['ENST00000345363', 'ENST00000396782', 'ENST00000396789', 'ENST00000646659', 'ENST00000646739', 'ENST00000647085'], 'Q9NPC1': ['ENST00000533293', 'ENST00000543919', 'ENST00000643080', 'ENST00000644309'], 'Q9Y271': ['ENST00000373304', 'ENST00000614798'], 'Q9NS75': ['ENST00000282018', 'ENST00000614739'], 'Q8TDS5': ['ENST00000378661'], 'Q92633': ['ENST00000358883', 'ENST00000374430', 'ENST00000374431'], 'Q9HBW0': ['ENST00000407877', 'ENST00000542587', 'ENST00000586703'], 'Q9UBY5': ['ENST00000370611', 'ENST00000440886'], 'Q99677': ['ENST00000435339', 'ENST00000614823'], 'Q9H1C0': ['ENST00000329858', 'ENST00000431922'], 'P43657': ['ENST00000345941', 'ENST00000378434', 'ENST00000620633'], 'P21453': ['ENST00000305352', 'ENST00000475289', 'ENST00000648480', 'ENST00000649383'], 'O95136': ['ENST00000646641'], 'Q99500': ['ENST00000358157', 'ENST00000375846'], 'O95977': ['ENST00000246115'], 'Q9H228': ['ENST00000333430', 'ENST00000439028'], 'P21554': ['ENST00000369499', 'ENST00000369501', 'ENST00000428600', 'ENST00000468898', 'ENST00000549890'], 'P34972': ['ENST00000374472'], 'Q14330': ['ENST00000340807', 'ENST00000397470', 'ENST00000397473'], 'Q9Y2T6': ['ENST00000392039', 'ENST00000392040', 'ENST00000444078', 'ENST00000622008', 'ENST00000650999'], 'Q8TDV5': ['ENST00000276218'], 'P25105': ['ENST00000305392', 'ENST00000373857', 'ENST00000539896'], 'Q13258': ['ENST00000306051', 'ENST00000553372'], 'Q9Y5Y4': ['ENST00000332539'], 'P34995': ['ENST00000292513'], 'P43116': ['ENST00000245457'], 'P43115': ['ENST00000306666', 'ENST00000351052', 'ENST00000356595', 'ENST00000370924', 'ENST00000370931', 'ENST00000460330', 'ENST00000479353', 'ENST00000628037'], 'P35408': ['ENST00000302472'], 'P43088': ['ENST00000370756', 'ENST00000370757', 'ENST00000370758'], 'P43119': ['ENST00000291294'], 'P21731': ['ENST00000375190', 'ENST00000411851'], 'P48039': ['ENST00000307161'], 'P49286': ['ENST00000257068'], 'P30542': ['ENST00000309502', 'ENST00000337894', 'ENST00000367235', 'ENST00000367236', 'ENST00000640524'], 'P29274': ['ENST00000337539', 'ENST00000610595', 'ENST00000611543', 'ENST00000618076'], 'P29275': ['ENST00000304222'], 'P0DMS8': ['ENST00000241356'], 'P47900': ['ENST00000305097'], 'P41231': ['ENST00000311131', 'ENST00000393596', 'ENST00000393597'], 'P51582': ['ENST00000374519'], 'Q15077': ['ENST00000349767', 'ENST00000393590', 'ENST00000393591', 'ENST00000393592', 'ENST00000538328', 'ENST00000540124', 'ENST00000540342', 'ENST00000542092', 'ENST00000618468'], 'Q96G91': ['ENST00000321826'], 'Q9H244': ['ENST00000302632'], 'Q9BPV8': ['ENST00000325602'], 'Q15391': ['ENST00000309170', 'ENST00000424796'], 'Q8TDU6': ['ENST00000479077', 'ENST00000519574', 'ENST00000521462', 'ENST00000522678'], 'Q99527': ['ENST00000297469', 'ENST00000397088', 'ENST00000397092', 'ENST00000401670'], 'Q9BXC0': ['ENST00000432564'], 'Q8TDS4': ['ENST00000328880'], 'P49019': ['ENST00000528880'], 'Q96P68': ['ENST00000298440', 'ENST00000543457'], 'Q9BXA5': ['ENST00000362032'], 'P08100': ['ENST00000296271'], 'Q9UHM6': ['ENST00000241891', 'ENST00000372071'], 'Q9H1Y3': ['ENST00000366554'], 'Q6U736': ['ENST00000371211', 'ENST00000638973'], 'P03999': ['ENST00000249389'], 'P04001': ['ENST00000595290'], 'P04000': ['ENST00000369951'], 'P46091': ['ENST00000407325', 'ENST00000437420', 'ENST00000612892', 'ENST00000621141', 'ENST00000636848', 'ENST00000637075', 'ENST00000638097', 'ENST00000638116'], 'P46089': ['ENST00000374024'], 'P46093': ['ENST00000323040'], 'P46095': ['ENST00000275169', 'ENST00000414000'], 'P47775': ['ENST00000381436', 'ENST00000405846'], 'P49685': ['ENST00000284311'], 'Q13304': ['ENST00000272644', 'ENST00000393018', 'ENST00000544369'], 'Q15760': ['ENST00000332427', 'ENST00000540510', 'ENST00000651487'], 'Q99678': ['ENST00000377741', 'ENST00000613887'], 'Q99679': ['ENST00000373642'], 'Q99680': ['ENST00000304402', 'ENST00000639742'], 'O00155': ['ENST00000304244'], 'Q8NDV2': ['ENST00000284674'], 'Q9NS67': ['ENST00000304411'], 'O00270': ['ENST00000366834'], 'O75388': ['ENST00000270590'], 'Q49SQ1': ['ENST00000399285'], 'Q9UPC5': ['ENST00000378138', 'ENST00000378142', 'ENST00000649219'], 'Q9HC97': ['ENST00000319838', 'ENST00000403859', 'ENST00000407714', 'ENST00000430267', 'ENST00000438013'], 'O15354': ['ENST00000303921'], 'O60883': ['ENST00000367282'], 'O43194': ['ENST00000329321'], 'Q9Y5Y3': ['ENST00000258456'], 'Q13585': ['ENST00000218316'], 'Q9Y2T5': ['ENST00000367685'], 'Q9BZJ8': ['ENST00000404129', 'ENST00000469383', 'ENST00000527748', 'ENST00000616874', 'ENST00000618721'], 'Q9BZJ7': ['ENST00000322241'], 'Q9BZJ6': ['ENST00000229955'], 'Q8IYL9': ['ENST00000267549'], 'Q15743': ['ENST00000531499', 'ENST00000535815', 'ENST00000650645'], 'O95800': ['ENST00000394705'], 'Q96P69': ['ENST00000382487'], 'Q96P67': ['ENST00000302548'], 'Q9NYM4': ['ENST00000243673'], 'Q9NQS5': ['ENST00000267015', 'ENST00000551809'], 'P60893': ['ENST00000297146', 'ENST00000424100', 'ENST00000449591', 'ENST00000610164'], 'Q9BY21': ['ENST00000260843'], 'Q9GZN0': ['ENST00000315033'], 'Q96P66': ['ENST00000298110', 'ENST00000651716'], 'Q9UNW8': ['ENST00000329797', 'ENST00000392585', 'ENST00000539291'], 'Q8IZ08': ['ENST00000395116', 'ENST00000481661'], 'Q6DWJ6': ['ENST00000570682'], 'Q7Z602': ['ENST00000334425', 'ENST00000447769'], 'Q7Z601': ['ENST00000335666'], 'Q96CH1': ['ENST00000397095', 'ENST00000444847'], 'Q8TDV2': ['ENST00000309926'], 'Q86SP6': ['ENST00000389740'], 'Q8NGU9': ['ENST00000380007'], 'Q8TDV0': ['ENST00000311104'], 'Q8TDT2': ['ENST00000312457'], 'Q6NV75': ['ENST00000377893'], 'Q9UJ42': ['ENST00000355897'], 'Q8N6U8': ['ENST00000367835', 'ENST00000367836', 'ENST00000367838', 'ENST00000537209', 'ENST00000539777', 'ENST00000546300'], 'Q16538': ['ENST00000311268', 'ENST00000428545'], 'O14626': ['ENST00000309180', 'ENST00000617554'], 'Q9NS66': ['ENST00000332582'], 'Q9BXC1': ['ENST00000276077', 'ENST00000645147'], 'Q14439': ['ENST00000299092', 'ENST00000543580', 'ENST00000561100'], 'O15218': ['ENST00000300098'], 'P32249': ['ENST00000376414'], 'Q9BXB1': ['ENST00000379214', 'ENST00000389858'], 'O75473': ['ENST00000266674', 'ENST00000536515', 'ENST00000540815'], 'Q9HBX8': ['ENST00000255432', 'ENST00000367278', 'ENST00000439764'], 'P04201': ['ENST00000252660', 'ENST00000674077'], 'P35410': ['ENST00000377127', 'ENST00000383549', 'ENST00000383643', 'ENST00000415802', 'ENST00000418003', 'ENST00000420035', 'ENST00000449044', 'ENST00000451019'], 'Q8TDS7': ['ENST00000309106'], 'Q86SM8': ['ENST00000389832'], 'Q96AM1': ['ENST00000309099', 'ENST00000441623'], 'Q86SM5': ['ENST00000332314'], 'Q96LB2': ['ENST00000302797'], 'Q96LB1': ['ENST00000329773'], 'Q96LB0': ['ENST00000396275', 'ENST00000621697'], 'Q96LA9': ['ENST00000314254'], 'Q86VZ1': ['ENST00000381297'], 'O00398': ['ENST00000171757', 'ENST00000544091'], 'Q9P1P5': ['ENST00000275191', 'ENST00000367931'], 'O14804': ['ENST00000258034'], 'Q96RI8': ['ENST00000275198'], 'Q969N4': ['ENST00000275200'], 'Q96RI9': ['ENST00000434551']}"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(transProtDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setList=set()\n",
    "for element in protList:\n",
    "    setList.add(element)\n",
    "len(setList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transProtDic.keys())\n",
    "len(protList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_index_disbool(old, new):\n",
    "    ''' Add a first column as an index and another one with T/F values to know if the variant\n",
    "    is related or not with a disease'''\n",
    "    columNamesNEW_with_index=[\"Number\",\"Family\",\"Name\",\"Short\",\"Uniprot ID\",\"Uniprot entry\",\"Struc\",\"gene\",\"varID\",\n",
    "               \"GPCRdb\",\"SequenceNumber\",\"Segment\",\"GPCRdbWT\",\"NMaa\",\"MutationType\",\n",
    "               \"Allele Count\", \"Allele Frequency\",\"Allele Number\",\"Role\",\"Impact Prediction\",\n",
    "                \"diseaseId\",\"score\",\"diseaseName\",\"related_diseases\",\"disease\"]\n",
    "    \n",
    "    count=0                          \n",
    "    with open(os.path.join(resultspath,old),\"r\") as myvarfile:\n",
    "        myvarread = csv.reader(myvarfile, delimiter=';') \n",
    "        with open(os.path.join(resultspath,new),\"w\") as myvarfile_pred: \n",
    "            mywriter = csv.writer(myvarfile_pred, delimiter=';') \n",
    "            for myrow in myvarread:\n",
    "                if myrow[0]==\"Family\": #first row, new names\n",
    "                    mywriter.writerow(columNamesNEW_with_index)\n",
    "                    continue            \n",
    "                else:\n",
    "                    mynewrow=myrow[:-2]\n",
    "                    mynewrow.insert(0, count)\n",
    "                    if myrow[20]!='?':\n",
    "                        mynewrow[-1]=1\n",
    "                    else: \n",
    "                        mynewrow[-1]=0\n",
    "                    mywriter.writerow(mynewrow)\n",
    "                count+=1\n",
    "# add_index_disbool('filtered_var_list/vars_with_impact.csv','filtered_var_list/vars_with_impact_index.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PTM information using PTMdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### how to? https://research.bioinformatics.udel.edu/iptmnet/about/api\n",
    "# http://dbptm.mbc.nctu.edu.tw/index.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant selection\n",
    "In order to select the variants to simulare we will add a total score to each of the,. It will consider:\n",
    "+ Frequency, considered via *Impact Scores*: \n",
    " + SIFT (https://sift.bii.a-star.edu.sg/www/nprot.2009.86.pdf): 'For a given protein sequence, SIFT compiles a dataset of functionally related  protein  sequences  by  searching  a  protein  database  using  the PSI-BLAST algorithm6. It then builds an alignment from the homo-logous sequences with the query sequence. In the second step of thealgorithm, SIFT scans each position in the alignment and calculates the probabilities  for  all  possible  20  amino  acids  at  that  position.  These probabilities are normalized by  the probability of the most frequent amino  acid  and  are  recorded  in  a  scaled  probability  matrix. [...]'.\n",
    " + Polyphen2 (http://genetics.bwh.harvard.edu/pph2/dokuwiki/overview): 'Elements of the matrix (profile scores) are logarithmic ratios of the likelihood of given amino acid occurring at a particular position to the likelihood of this amino acid occurring at any position (background frequency). [...]'\n",
    " So , we will consider the variants with score 2, tagged as deleterious by both impact predicting scores.\n",
    "\n",
    "+ If they are related with a disease.\n",
    "\n",
    "+ PDB structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_vars(file_vars):\n",
    "    ''' Read the file where the variants with index are defined and filter the ones\n",
    "    with score 2, one pdb structure, related with a disease'''\n",
    "    vars_dic={}\n",
    "    with open(os.path.join(resultspath,file_vars),\"r\") as myvarfile:\n",
    "        myvarread = csv.reader(myvarfile, delimiter=';') \n",
    "        for myrow in myvarread:\n",
    "            score=0\n",
    "            if myrow[0]=='Number':\n",
    "                continue\n",
    "            else:\n",
    "                if myrow[19]=='2':#impact prediction\n",
    "                    score+=10\n",
    "                if myrow[6]!= '':#pdb structure\n",
    "                    score+=10\n",
    "                if myrow[23]!= '?':#related with disease\n",
    "                    score+=int(myrow[23])\n",
    "            if score>19:\n",
    "                newrow=myrow\n",
    "                newrow.append(score)\n",
    "                vars_dic[myrow[8]]=newrow#key entry name, value the row of the excel\n",
    "    return vars_dic\n",
    "selected_variants= select_vars('filtered_var_list/vars_with_impact_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for key, value in selected_variants.items():\n",
    "#     if value[-1]>20:\n",
    "#         print(value[5],value[8],value[-1])\n",
    "len(selected_variants.keys() ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPCR selection\n",
    "### Compute total score\n",
    "In order to select the GPCRs to simulate we will add a total score to each GPCR. It will be computed as:\n",
    "\n",
    "+ +1 if the allele frequency is avobe the mean on the obtained frequencies.\n",
    "\n",
    "+ +1 If it has variants in more than one of the selected positions (we have 5 positions so here the max score is 4).\n",
    "\n",
    "+ +0.5 If it is related with one disease.\n",
    "\n",
    "+ Consensus impact score: +0 if it is 'Benign', +1 if it is considered 'deleterious' by one predicting tool and +2 if it is predicted as 'deleterious' by both tools.\n",
    "\n",
    "*** We could also consider if the variant is located in a PTM but we don't know how to do it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5ht1f_human 12x48 ICL1 2\n",
      "5ht1f_human 12x48 ICL1 4\n",
      "5ht2b_human 6x29 TM6 2\n",
      "5ht2b_human 4x39 TM4 4\n",
      "5ht2b_human 4x39 TM4 5\n",
      "5ht5a_human 12x48 ICL1 2\n",
      "*** bkrb2_human 3\n",
      "*** c3ar_human 3\n",
      "*** c3ar_human 3\n",
      "galr2_human 23x49 ECL1 2\n",
      "kissr_human 3x50 TM3 2\n",
      "*** nmur1_human 3\n",
      "*** nmur1_human 3\n",
      "*** npsr1_human 3\n",
      "oprm_human 12x48 ICL1 3.0\n",
      "oprm_human 12x48 ICL1 7.0\n",
      "oprm_human 3x50 TM3 8.0\n",
      "rxfp1_human 6x29 TM6 2\n",
      "ssr1_human 12x48 ICL1 2\n",
      "ssr4_human 3x50 TM3 3\n",
      "ssr4_human 3x50 TM3 5\n",
      "*** ssr4_human 3\n",
      "ssr4_human 6x29 TM6 7\n",
      "ssr5_human 3x50 TM3 3\n",
      "ssr5_human 3x50 TM3 5\n",
      "*** cml1_human 3\n",
      "cxcr1_human 3x50 TM3 2\n",
      "cxcr1_human 12x48 ICL1 3\n",
      "cxcr2_human 3x50 TM3 2\n",
      "xcr1_human 3x50 TM3 2\n",
      "xcr1_human 3x50 TM3 4\n",
      "ackr1_human 4x39 TM4 2\n",
      "ackr3_human 3x50 TM3 2\n",
      "*** oxer1_human 3\n",
      "*** fpr2_human 3\n",
      "*** s1pr4_human 3\n",
      "ptafr_human 4x39 TM4 2\n",
      "*** mtr1b_human 3\n",
      "aa2ar_human 3x50 TM3 2\n",
      "aa2ar_human 3x50 TM3 4\n",
      "aa2ar_human 4x39 TM4 6\n",
      "*** p2ry6_human 3\n",
      "opn5_human 23x49 ECL1 2\n",
      "gpr17_human 3x50 TM3 2\n",
      "gpr31_human 3x50 TM3 2\n",
      "gpr31_human 3x50 TM3 4\n",
      "gpr63_human 3x50 TM3 2\n",
      "gpr63_human 23x49 ECL1 3\n",
      "gpr78_human 6x29 TM6 3\n",
      "gp142_human 3x50 TM3 3\n",
      "gp142_human 3x50 TM3 6\n",
      "gp142_human 6x29 TM6 7\n",
      "gp142_human 6x29 TM6 7\n",
      "lgr5_human 3x50 TM3 2\n",
      "opn5_human 23x49 ECL1 5\n"
     ]
    }
   ],
   "source": [
    "def first_row(rowValues):\n",
    "    '''Given a row, add +1 if the allele freq is avobe the mean, +0.5 if related with a \n",
    "    disease, return the final score and also the postion'''\n",
    "    scoreRow=0\n",
    "    freqMean=0.00045013584139848024# obatined in variants_info_plots.ipynb\n",
    "    if rowValues[23]!='?': #related with a disease +0.5\n",
    "        scoreRow+=int(rowValues[23])/2\n",
    "    if float(rowValues[16])>freqMean:# high frequency\n",
    "        scoreRow+=1\n",
    "    if rowValues[19]:\n",
    "        scoreRow+=int(rowValues[19])# impact score\n",
    "    position=rowValues[9]\n",
    "    dataList=[scoreRow,position]\n",
    "    return dataList\n",
    "\n",
    "\n",
    "def total_score(old,new):\n",
    "    '''Add final column with the total score for each GPCR. Print the GPCRs that \n",
    "    have an score avobe the mean + one strandard deviation (computed in 'variants_info_plots.ipynb'\n",
    "    with that last column) and that have at least two states in PDB. Build a dict with the GPCRs with high total 'simulation' score \n",
    "    '''\n",
    "    highImpact={}\n",
    "    with open(os.path.join(resultspath,old),\"r\") as myvarfile:\n",
    "        myvarread = csv.reader(myvarfile, delimiter=';') \n",
    "        with open(os.path.join(resultspath,new),\"w\") as myvarfile_pred: \n",
    "            mywriter = csv.writer(myvarfile_pred, delimiter=';') \n",
    "            scoreDic={}\n",
    "            for myrow in myvarread:\n",
    "                if myrow[0]==\"Number\": #first row, new names\n",
    "                    mynewrow=myrow\n",
    "                    mynewrow.append('total_score')\n",
    "                    mywriter.writerow(mynewrow)\n",
    "                    continue            \n",
    "                else:\n",
    "                    entry=myrow[5]                    \n",
    "                    mynewrow=myrow\n",
    "                    \n",
    "                    if entry not in scoreDic.keys(): #the entry name is new, first we consider the 1st row\n",
    "                        positions=set()\n",
    "                        scorePos=first_row(myrow)#score of the row, first position\n",
    "                        positions.add(scorePos[1])\n",
    "                        numPos=1\n",
    "                        scoreDic[entry]=scorePos[0]\n",
    "                    else:#the entry name has already an analyzed row\n",
    "                        scorePos=first_row(myrow)\n",
    "                        positions.add(scorePos[1])\n",
    "                        newNumPos=len(positions)\n",
    "                        if newNumPos!=numPos:#new position +1\n",
    "                            scoreDic[entry]+=1\n",
    "                            numPos=newNumPos\n",
    "                        if newNumPos==3:# to select one to be displayed\n",
    "                            print ('***',entry, newNumPos)\n",
    "                        scoreDic[entry]+=scorePos[0]\n",
    "                    # mean +2 sigma fo the total score (last colum of the created file)\n",
    "                    #and two PDBs, computed with the df of variants_info_plots.ipynb\n",
    "#                     if scoreDic[entry]>2.3+2.15*2 and myrow[6].count('&')>0:\n",
    "                    if scoreDic[entry]>1 and myrow[6].count('&')>0: #mean \n",
    "                        print(entry,myrow[9],  myrow[11],scoreDic[entry])\n",
    "                        highImpact[entry]=myrow\n",
    "                    mynewrow.append(scoreDic[entry])#entry_name\n",
    "                    mywriter.writerow(mynewrow)\n",
    "                    \n",
    "    return (highImpact)\n",
    "     \n",
    "possibleGPCRs=total_score('filtered_var_list/vars_with_impact_index.csv','filtered_var_list/vars_with_total_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['5ht1f_human', '5ht2b_human', '5ht5a_human', 'galr2_human', 'kissr_human', 'oprm_human', 'rxfp1_human', 'ssr1_human', 'ssr4_human', 'ssr5_human', 'cxcr1_human', 'cxcr2_human', 'xcr1_human', 'ackr1_human', 'ackr3_human', 'ptafr_human', 'aa2ar_human', 'opn5_human', 'gpr17_human', 'gpr31_human', 'gpr63_human', 'gpr78_human', 'gp142_human', 'lgr5_human'])\n"
     ]
    }
   ],
   "source": [
    "print(possibleGPCRs.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if they are GPCRs are drug targets:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5820538/ (excel downloaded in data/Studied_GPCR_vars/Targets-Drugs_table/Targets-Drugs.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_targets(file_tagets):\n",
    "    ''' Read the file where the drug targets are classified and build the dictionary'''\n",
    "    target_dic={'consensus':[], 'not-consensus':[], 'not-consensus*':[],'not-target':[]}\n",
    "    with open(os.path.join(datapath,file_tagets),\"r\") as myvarfile:\n",
    "        myvarread = csv.reader(myvarfile, delimiter=',') \n",
    "        for myrow in myvarread:\n",
    "            if myrow[0]=='Consensus': # Confirmed drug target by all data sources\n",
    "                target_dic['consensus'].append(' '+myrow[1][0:4])\n",
    "            elif myrow[0]=='Not-consensus':# Confirmed drug target by not all data sources\n",
    "                target_dic['not-consensus'].append(' '+myrow[1][0:4])\n",
    "            elif myrow[0]=='Not-consensus, with comments': #Confirmed drug target by few data sources\n",
    "                target_dic['not-consensus*'].append(' '+myrow[1][0:4])\n",
    "            elif myrow[0]=='Not drug target':\n",
    "                target_dic['not-target'].append(' '+myrow[1][0:4])\n",
    "    return target_dic\n",
    "targets=read_targets('Targets-Drugs_table/Targets-Drugs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'consensus': [' ADOR',\n",
       "  ' ADOR',\n",
       "  ' ADOR',\n",
       "  ' ADOR',\n",
       "  ' ADRA',\n",
       "  ' ADRA',\n",
       "  ' ADRA',\n",
       "  ' ADRA',\n",
       "  ' ADRA',\n",
       "  ' ADRA',\n",
       "  ' ADRB',\n",
       "  ' ADRB',\n",
       "  ' ADRB',\n",
       "  ' AGTR',\n",
       "  ' AVPR',\n",
       "  ' AVPR',\n",
       "  ' AVPR',\n",
       "  ' BDKR',\n",
       "  ' CASR',\n",
       "  ' CCR5',\n",
       "  ' CHRM',\n",
       "  ' CHRM',\n",
       "  ' CHRM',\n",
       "  ' CNR1',\n",
       "  ' CXCR',\n",
       "  ' CYSL',\n",
       "  ' DRD1',\n",
       "  ' DRD2',\n",
       "  ' DRD3',\n",
       "  ' DRD4',\n",
       "  ' DRD5',\n",
       "  ' EDNR',\n",
       "  ' EDNR',\n",
       "  ' F2R',\n",
       "  ' FSHR',\n",
       "  ' GABB',\n",
       "  ' GABB',\n",
       "  ' GLP1',\n",
       "  ' GNRH',\n",
       "  ' HCRT',\n",
       "  ' HCRT',\n",
       "  ' HRH1',\n",
       "  ' HRH2',\n",
       "  ' HTR1',\n",
       "  ' HTR1',\n",
       "  ' HTR1',\n",
       "  ' HTR1',\n",
       "  ' HTR2',\n",
       "  ' HTR2',\n",
       "  ' HTR2',\n",
       "  ' HTR4',\n",
       "  ' LHCG',\n",
       "  ' MTNR',\n",
       "  ' MTNR',\n",
       "  ' OPRD',\n",
       "  ' OPRK',\n",
       "  ' OPRM',\n",
       "  ' OXTR',\n",
       "  ' P2RY',\n",
       "  ' PTGE',\n",
       "  ' PTGE',\n",
       "  ' PTGE',\n",
       "  ' PTGE',\n",
       "  ' PTGF',\n",
       "  ' PTGI',\n",
       "  ' S1PR',\n",
       "  ' S1PR',\n",
       "  ' SMO',\n",
       "  ' SSTR',\n",
       "  ' SSTR',\n",
       "  ' SSTR',\n",
       "  ' SSTR',\n",
       "  ' TACR'],\n",
       " 'not-consensus': [' CALC',\n",
       "  ' CHRM',\n",
       "  ' CHRM',\n",
       "  ' CNR2',\n",
       "  ' CYSL',\n",
       "  ' FFAR',\n",
       "  ' FPR1',\n",
       "  ' GCGR',\n",
       "  ' GHRH',\n",
       "  ' GLP2',\n",
       "  ' HCAR',\n",
       "  ' HRH4',\n",
       "  ' HTR1',\n",
       "  ' HTR5',\n",
       "  ' HTR6',\n",
       "  ' HTR7',\n",
       "  ' MC2R',\n",
       "  ' MLNR',\n",
       "  ' NTSR',\n",
       "  ' P2RY',\n",
       "  ' PTGD',\n",
       "  ' PTGD',\n",
       "  ' PTH1',\n",
       "  ' SCTR',\n",
       "  ' TAAR',\n",
       "  ' TBXA',\n",
       "  ' TSHR',\n",
       "  ' HRH3',\n",
       "  ' TRHR',\n",
       "  ' GPR5',\n",
       "  ' GPER',\n",
       "  ' GPR3',\n",
       "  ' GPBA'],\n",
       " 'not-consensus*': [' GPR1',\n",
       "  ' HCAR',\n",
       "  ' PTH2',\n",
       "  ' P2RY',\n",
       "  ' P2RY',\n",
       "  ' P2RY',\n",
       "  ' P2RY',\n",
       "  ' SUCN',\n",
       "  ' ACKR',\n",
       "  ' CCR4',\n",
       "  ' CCKB',\n",
       "  ' HCAR',\n",
       "  ' GPR1',\n",
       "  ' BDKR',\n",
       "  ' MRGP',\n",
       "  ' CCKA',\n",
       "  ' MC1R',\n",
       "  ' NPY4',\n",
       "  ' GPR6',\n",
       "  ' S1PR',\n",
       "  ' S1PR',\n",
       "  ' S1PR',\n",
       "  ' ADGR',\n",
       "  ' MC5R',\n",
       "  ' MC3R',\n",
       "  ' MC4R',\n",
       "  ' CRHR',\n",
       "  ' SSTR'],\n",
       " 'not-target': [' NPSR',\n",
       "  ' NPY2',\n",
       "  ' AGTR',\n",
       "  ' GPRC',\n",
       "  ' CXCR',\n",
       "  ' GNRH',\n",
       "  ' CCR2',\n",
       "  ' CRHR',\n",
       "  ' FFAR',\n",
       "  ' GPR1',\n",
       "  ' NPY1',\n",
       "  ' P2RY',\n",
       "  ' PTAF',\n",
       "  ' GPRC',\n",
       "  ' GRM2',\n",
       "  ' GRM3',\n",
       "  ' GRM6',\n",
       "  ' GRM1',\n",
       "  ' GRM4',\n",
       "  ' GRM5',\n",
       "  ' GRM7',\n",
       "  ' GRM8',\n",
       "  ' TAS1',\n",
       "  ' ACKR',\n",
       "  ' ACKR',\n",
       "  ' ACKR',\n",
       "  ' ADCY',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' ADGR',\n",
       "  ' APLN',\n",
       "  ' BRS3',\n",
       "  ' C3AR',\n",
       "  ' C5AR',\n",
       "  ' C5AR',\n",
       "  ' CALC',\n",
       "  ' CCR1',\n",
       "  ' CCR1',\n",
       "  ' CCR3',\n",
       "  ' CCR6',\n",
       "  ' CCR7',\n",
       "  ' CCR8',\n",
       "  ' CCR9',\n",
       "  ' CCRL',\n",
       "  ' CELS',\n",
       "  ' CELS',\n",
       "  ' CELS',\n",
       "  ' CMKL',\n",
       "  ' CX3C',\n",
       "  ' CXCR',\n",
       "  ' CXCR',\n",
       "  ' CXCR',\n",
       "  ' CXCR',\n",
       "  ' F2RL',\n",
       "  ' F2RL',\n",
       "  ' F2RL',\n",
       "  ' FFAR',\n",
       "  ' FFAR',\n",
       "  ' FPR2',\n",
       "  ' FPR3',\n",
       "  ' FZD1',\n",
       "  ' FZD1',\n",
       "  ' FZD2',\n",
       "  ' FZD3',\n",
       "  ' FZD4',\n",
       "  ' FZD5',\n",
       "  ' FZD6',\n",
       "  ' FZD7',\n",
       "  ' FZD8',\n",
       "  ' FZD9',\n",
       "  ' GALR',\n",
       "  ' GALR',\n",
       "  ' GALR',\n",
       "  ' GHSR',\n",
       "  ' GIPR',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR1',\n",
       "  ' GPR2',\n",
       "  ' GPR2',\n",
       "  ' GPR2',\n",
       "  ' GPR2',\n",
       "  ' GPR2',\n",
       "  ' GPR2',\n",
       "  ' GPR3',\n",
       "  ' GPR3',\n",
       "  ' GPR3',\n",
       "  ' GPR3',\n",
       "  ' GPR3',\n",
       "  ' GPR3',\n",
       "  ' GPR3',\n",
       "  ' GPR3',\n",
       "  ' GPR4',\n",
       "  ' GPR4',\n",
       "  ' GPR4',\n",
       "  ' GPR5',\n",
       "  ' GPR5',\n",
       "  ' GPR6',\n",
       "  ' GPR6',\n",
       "  ' GPR6',\n",
       "  ' GPR6',\n",
       "  ' GPR6',\n",
       "  ' GPR7',\n",
       "  ' GPR7',\n",
       "  ' GPR7',\n",
       "  ' GPR8',\n",
       "  ' GPR8',\n",
       "  ' GPR8',\n",
       "  ' GPR8',\n",
       "  ' GPR8',\n",
       "  ' GPR8',\n",
       "  ' GPRC',\n",
       "  ' GPRC',\n",
       "  ' GPRC',\n",
       "  ' GRPR',\n",
       "  ' HTR5',\n",
       "  ' KISS',\n",
       "  ' LGR4',\n",
       "  ' LGR5',\n",
       "  ' LGR6',\n",
       "  ' LPAR',\n",
       "  ' LPAR',\n",
       "  ' LPAR',\n",
       "  ' LPAR',\n",
       "  ' LPAR',\n",
       "  ' LPAR',\n",
       "  ' LTB4',\n",
       "  ' LTB4',\n",
       "  ' MAS1',\n",
       "  ' MAS1',\n",
       "  ' MCHR',\n",
       "  ' MCHR',\n",
       "  ' MRGP',\n",
       "  ' MRGP',\n",
       "  ' MRGP',\n",
       "  ' MRGP',\n",
       "  ' MRGP',\n",
       "  ' MRGP',\n",
       "  ' MRGP',\n",
       "  ' NMBR',\n",
       "  ' NMUR',\n",
       "  ' NMUR',\n",
       "  ' NPBW',\n",
       "  ' NPBW',\n",
       "  ' NPFF',\n",
       "  ' NPFF',\n",
       "  ' NPY5',\n",
       "  ' NPY6',\n",
       "  ' NTSR',\n",
       "  ' OPN3',\n",
       "  ' OPN4',\n",
       "  ' OPN5',\n",
       "  ' OPRL',\n",
       "  ' OR51',\n",
       "  ' OXER',\n",
       "  ' OXGR',\n",
       "  ' P2RY',\n",
       "  ' P2RY',\n",
       "  ' P2RY',\n",
       "  ' PRLH',\n",
       "  ' PROK',\n",
       "  ' PROK',\n",
       "  ' QRFP',\n",
       "  ' RXFP',\n",
       "  ' RXFP',\n",
       "  ' RXFP',\n",
       "  ' RXFP',\n",
       "  ' TAAR',\n",
       "  ' TAAR',\n",
       "  ' TAAR',\n",
       "  ' TAAR',\n",
       "  ' TAAR',\n",
       "  ' TAAR',\n",
       "  ' TAAR',\n",
       "  ' TACR',\n",
       "  ' TACR',\n",
       "  ' TAS1',\n",
       "  ' TAS1',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TAS2',\n",
       "  ' TPRA',\n",
       "  ' UTS2',\n",
       "  ' VIPR',\n",
       "  ' VIPR',\n",
       "  ' XCR1']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in the file 5HT1F\n",
      "Not in the file 5HT2B\n",
      "Not in the file 5HT5A\n",
      "Not in the file GALR2\n",
      "Not in the file KISSR\n",
      "Drug target OPRM\n",
      "Not in the file RXFP1\n",
      "Not in the file SSR1\n",
      "Not in the file SSR4\n",
      "Not in the file SSR5\n",
      "Not in the file CXCR1\n",
      "Not in the file CXCR2\n",
      "Not drug target XCR1\n",
      "Not in the file ACKR1\n",
      "Not in the file ACKR3\n",
      "Not in the file PTAFR\n",
      "Not in the file AA2AR\n",
      "Not drug target OPN5\n",
      "Not in the file GPR17\n",
      "Not in the file GPR31\n",
      "Not in the file GPR63\n",
      "Not in the file GPR78\n",
      "Not in the file GP142\n",
      "Not drug target LGR5\n"
     ]
    }
   ],
   "source": [
    "for gpcrs in possibleGPCRs.keys():\n",
    "#     print(gpcrs)\n",
    "    gpcrs=gpcrs.upper()\n",
    "    gpcr=gpcrs[:-6]\n",
    "#     print(gpcr)\n",
    "    if targets['consensus'].count(' '+gpcr)>0:\n",
    "        print('Drug target', gpcr)\n",
    "    elif targets['not-consensus'].count(' '+gpcr)>0 or targets['not-consensus*'].count(' '+gpcr)>0:\n",
    "        print('Possible drug target', gpcr)\n",
    "    elif targets['not-target'].count(' '+gpcr)>0:\n",
    "        print('Not drug target', gpcr)\n",
    "    else:\n",
    "        print('Not in the file', gpcr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
